[
["index.html", "Data Exploration of U.S. Police Stops Chapter 1 Preface 1.1 Dedication 1.2 Motivation", " Data Exploration of U.S. Police Stops Pomona College Data Science Research Circle Spring 2020 Chapter 1 Preface 1.1 Dedication First, we affirm strongly that #BlackLivesMatter as our project (which attempts to analyze police behavior in the US) comes to a close in May 2020. As institutional injustices committed against Black and Indigenous people of color (BIPOC) persist, we hope not to center ourselves in their activism. Rather, we affirm the movement as a reminder for fellow students and academics who work with traffic stop data and are almost exclusively non-BIPOC. Critically reflect on our role as (budding) data scientists, statisticians, economists, criminologists: how has privilege enabled us to work in this field? And, how has privilege limited our lived experiences to be several degrees removed from the goals of our research? How do we try to resolve this discrepancy? Second, our team would like to thank our generous and kind professors, Professor Hardin and Professor Sarkis, for providing the expertise, encouragement, and wonderful banter that helped us through this unprecedented and strenuous semester. We are grateful for your guidance and will miss our Tuesday, 8:00 am Zoom calls. 1.2 Motivation Welcome! This page is the manifestation of a semester-long investigative project conducted by a Data Science Research Circle at Pomona College in Claremont, CA. The research circle consisted of seven students including: Amber Lee (‘22), Arm Wonghirundacha (‘22), Emma Godfrey (‘21), Ethan Ong (‘21), Ivy Yuan (‘21), Oliver Chang (‘22), and Will Gray (‘22). Additionally the research circle was advised by Professors Jo Hardin and Ghassan Sarkis from the Department of Mathematics. The work is motivated by recent literature and investigative analysis on policing practices around the country, particularly from the Stanford Open Policing Project. The Stanford Open Policing Project is an ongoing research effort to gather and publish police stop data from around the United States. The project is a collaboration between the Stanford Computational Journalism Lab and the Stanford School of Engineering led by professor Cheryl Phillips and Sharad Goel. Beginning in 2015, the Stanford Open Policing project has requested both state-wide and city-wide public information about police stops; requests were filed for all 50 states and over 100 cities. On June 19th 2017, the project published the collected data on their website openpolicing.stanford.edu and as of 2020, the website holds data from 42 states totalling 255 million data points. 221 million of these stops were from 33 state patrol datasets collected and the remaining 34 million were from 56 city-wide datasets (Pierson et al. (2020)). One of the primary focuses of the Stanford Open Policing Project is traffic stop data. More than 50,000 traffic stops occur every day — 20 million in a year — yet prior to the debut of the Open Policing Project database, there was a lack of reliable, comprehensive, and standardized national datasets on these traffic stop data to analyze. The Open Policing Project provides a unique opportunity to investigate these traffic stops, with particular intention to work towards a definitive test of racial profiling. We hope to extend the interdisciplinary work towards this difinitive test—a test grounded in social and political relevance. As expressed by Goel et al. (2017), knowing the true extent of racial profiling is a crucial tool for informing police department trainings, protecting Fourth Amendment rights of minority defendants, and validating the qualitative experiences of historically-oppressed groups. The motivation for this project is to continue the analysis of the Stanford Open Policing Project dataset. This includes reproducing previous findings and applying methodologies used on smaller datasets to the Stanford nation-wide database. The three topics of interest before beginning EDA for this project were the following: the enactment of seatbelt laws and its effect on traffic stops, night versus day stops, and discrepancies of search rates. Not all were pursued, but each provided a unique direction for data analysis. The report is organized into several parts: I. Preface, II. Literature Review, III. Functions. IV. Description of Data, V. Exploratory Data Analysis, VI. Logistic Regression, and VI. Limitations and Discussion. References "],
["literature-review.html", "Chapter 2 Literature Review", " Chapter 2 Literature Review As both traffic and pedestrian stop data have become more readily available to the public, researchers have explored novel methods of testing for discrimination. The fundamental problem of traffic stop data is the lack of accompanying traffic data – although we know who was stopped in a given time and location, we do not know the fellow motorists of this unlucky motorist who was pulled over. If we consistently find that this unlucky motorist is black or Hispanic/Latinx while his/her/their fellow motorists are white, then this may provide evidence of racial profiling. However, the demographic information of these fellow motorists remains unknown. The lack of a denominator (which would convey who else is driving at a given time and location) has motivated a range of attempts to circumvent the baseline driving population through natural experiments, experimental designs, and analyses of post-stop outcomes. As researchers and social scientists continue to drive the progress of assessing racial bias in police practices around the country, it is important to note that a majority of the current methods of assessments do not meet the basic assumptions of causal inference. Ridgeway and MacDonald (2010) discusses benchmarks (external vs internal) and how various analyses use them as a tool to assess the prevalence of racial bias but in fact may be responsible either hiding evidence of or exaggerating the practice. First, consider the use of external benchmarks. External benchmarks can be understood as concrete observations of traffic stops. In this study, every variable in the dataset describing an observation is an external benchmark. External benchmarks are useful because they enable an analysis to estimate the at-risk population distribution in terms of traffic stops. As a result, external benchmarks can be useful in building a robust model, however, it can also overlook indirect factors that may motivate the manifestation of what many are calling police discrimination. Ridgeway points out that internal benchmarks are an essential consideration as well in assessing racial bias in policing practices. In the context of traffic stops, internal benchmarks can be understood as thresholds or conditions that individual police officers may have that determine whether a stop happens or the stop outcome. In fact, considering such motivations is important in deciphering if racial bias is systematic or more in fact prevalent on an individual officer basis. Furthermore, in some cases racial differences did not depend on the race of the driver. From an audit of 313 vehicle-mounted video and audio recordings of the Cincinnati Police Department, the policing practices were more partisan based on the race of the police officer which contributes to the claim that policing racial bias may be more individualized than is communicated. From understanding how different benchmarks can be used to assess if policing bodies are racially bias, it is increasingly clear that developing more definitive tests of racial profiling continues to be a work in progress. In fact, it’s social and political relevance has been present in the United States for the past few decades. From cases such as Terry v. Ohio the negotiation of protecting Fourth Amendment rights of minority defendants has been an ongoing area of interest. Goel et al. (2017) introduces the idea of taking advantage of ‘the second information age’, leveraging big data and statistical modeling to find statistical evidence of these racial disparities, validating the various qualitative anecdotes heard around the country of oppression, brutality and injustice. The enactment of seatbelt laws presented a natural experiment in the context of traffic stops. With an analysis of stops pre and post legislative change, we can compare the magnitude to which the legislation affects the number of stops for different racial groups. Riddell et al. (2020) performed this study looking at South Carolina’s 2006 seat belt law, which enabled police troopers to conduct traffic stops for drivers who aren’t wearing a seat belt. Traffic stops conducted due to an observed violation (as opposed to due to speeding) increased between 2005 and 2006. For white and Hispanic drivers, this was a 50% increase; for Black drivers, this was a 58% percent increase. However, because the number of traffic stops changed, Black arrest rates decreased while Hispanic arrest rates increased. Furthermore, taking advantage of natural variation of traffic patterns between daytime and nighttime has shown to have an effect on traffic patterns. Kalinowski, Ross, and Ross (2019) found that the types of stops made by the police vary considerably between daytime and night-time. When daytime transforms into night-time, the number of speeding stops is reduced, while the number of equipment warning stops is increased. There also seems to exist a location shift of traffic stops, which indicates that the geographical distribution of police officers changes throughout the course of a day — some regions enjoy a higher share of manpower during the daytime, whereas some other regions enjoy a higher share of manpower during the night-time. It was also observed that counties receiving more police resources during the night-time have a higher share of minority residents. Discrepancies of search rates became of interest through the analysis of stop-and-frisk rates. Goel et al. (2016) studied the varying hit rates dependent on race of the driver with particular focus on suspicion of criminal possession of a weapon (CPW) stops. The results indicate a predominantly black and Hispanic demographic for incorrect presumption for a stop — correct hits accounted for 2.5% and 3.6% of CPW stops for black and Hispanic suspects respectively and 11% for white suspects. This discrepancy may suggest differences in the likelihood of stop and search for each demographic due to biased levels of suspicion. Much of previous literature including Grogger and Ridgeway (2006), Taniguchi et al. (2017), and Pierson et al. (2020) introduce logistic regression as an effective method of predicting traffic stop outcomes from the variables available. All incorporate different numbers of variables in order to calculate the probability of a driver stopped in the context of the veil of darkness (VOD) model, specifically drivers grouped by race. Additionally, something to consider is the notion of differing police behaviors during night and day can also be factored into logistic regression. Another natural experiment opportunity results with marijuana legalization and changes in enforcement of driving infractions, namely seat belt laws. Riddell 2020 proposes using seat belt laws to overcome the unknown denominator which has been pointed out by previous literature. The former results in less stops, the latter results in more stops. Either way, we can compare the magnitude to which these legislative changes affect the number of stops for different racial groups. Another body of literature focuses on pedestrian stops, which has the benefit of being more detailed with the location of the stop and the type of location that the stop is. There are less studies in this category probably due to limited data, but for the data that does exist, it is quite extensive Goel et al. (2016) Hannon (2019). We see a similar baseline problem of – we do not know who the fellow pedestrians are and the issue of consistent data collection is also apparent. Lastly, the last category of literature we have considered is literature concerning the racial disparities in traffic stop outcomes. Shoub, Baumgartner, and Epp (2017) explores how greater black political representation has an inhibitory effect on what happens during a stop and after a stop when compared with other less black represented districts. Even more interesting is that Rosenfeld, Rojek, and Decker (2012) explores how considering age in combination with race significantly shows disparities in traffic stop activity and outcomes. This is the primary focus of this project’s exploratory nature. References "],
["functions.html", "Chapter 3 Functions 3.1 Data Uploading Functions 3.2 Veil of Darkness Functions 3.3 Nationwide Functions", " Chapter 3 Functions This section explains the various functions used throughout our research. 3.1 Data Uploading Functions The functions here serve as cleaning functions such that a ready to use dataset gets uploaded to the database as a data table. convertLogicalToInt function: input: dataset output: dataset with logical values set to binary convertLogicalToInt &lt;- function(input_df){ vars &lt;- sapply(input_df, class) %&gt;% list.which(.==&#39;logical&#39;) input_df[,vars] &lt;- sapply(input_df[,vars], as.numeric) input_df return(input_df) } getDFName: input: name of dataset (this could also be name of link from web scrape) output: clean dataset name that includes state and city getDFName &lt;- function(input_string){ tmp &lt;- str_split(input_string, &#39;_&#39;, simplify = TRUE) tmp state &lt;- str_to_upper(tmp[,2]) if(tmp[,4] != &quot;2019&quot;){ cityName &lt;- paste(tmp[,3], tmp[,4], sep=&quot;&quot;) name &lt;- paste(state, cityName, sep=&quot;&quot;) } else { cityName &lt;- tmp[,3] name &lt;- paste(state, cityName, sep=&quot;&quot;) } name return(name) } uploadLinksToDatabase: input: string link from web scrape outout: 0 - indicating that the code finished executing This function will take in a RDS link and write the dataset to the database uploadLinksToDatabase &lt;- function(input_link){ tmpDF &lt;- readRDS(gzcon(url(input_link))) tmpDF &lt;- convertLogicalToInt(tmpDF) name&lt;- getDFName(input_link) dbWriteTable(con2, name, tmpDF, overwrite = TRUE) return(0) } 3.2 Veil of Darkness Functions Veil of Darkness functions work with lutz and lubridate to determine whether a stop took place in the dark. The first step in getting the sunset and sunrise times is to get the coordinates for the city. To do this, we can Google search the city name and webscrape the Google search results. The two code chunk below has functions that will perform this process. get_cityNames takes in a datatable’s name and cleans it so that the google search engine result will return the desired page that has the coordinates. get_cityNames &lt;- function(name){ check &lt;- str_extract(name, &quot;[a-z]+&quot;) if(check == &quot;statewide&quot;){ return(state.name[grep(str_sub(name, 1,2), state.abb)]) } else { return(check) } } get_coordinates takes in said clean city names and scrape the google search web result. The function will return a vector of doubles. The first index is for latitude and second index has longitude. get_coordinates &lt;- function(city){ url &lt;- paste(&quot;https://www.google.com/search?q=&quot;, city, &quot;+lat+long&amp;oq=sandiego+lat+long&amp;aqs=chrome..69i57.2463j0j7&amp;sourceid=chrome&amp;ie=UTF-8&quot;) doc &lt;- htmlParse(readLines(url), asText=TRUE) links &lt;- xpathSApply(doc, &quot;//div[@class=&#39;kCrYT&#39;]&quot;, xmlValue) clean_coor &lt;- as.list(str_split(links[2], &quot;,&quot;)) lat &lt;- as.numeric(str_extract(clean_coor[[1]][1], &quot;\\\\d+\\\\.*\\\\d*&quot;)) long &lt;- -1*as.numeric(str_extract(clean_coor[[1]][2], &quot;\\\\d+\\\\.*\\\\d*&quot;)) x &lt;- c(lat,long) return(x) } clean_names &lt;- list() clean_names &lt;- lapply(datasets_of_interest, get_cityNames) coordinates &lt;- lapply(clean_names,get_coordinates) Next, we will define two helper functions in getting that call the lutz package and retrieve the times. outsunriseset input: latitude (dbl), longitude(dbl), date(Date or Posix), timezime (tz), direction(string) output: Date with time of the desired sun direction oursunriseset &lt;- function(latitude, longitude, date, timezone, direction) { date.lat.long &lt;- data.frame(date = date, lat = latitude, lon = longitude) if(direction == &quot;sunset&quot;){ # call getSunlightTimes from the lutz package getSunlightTimes(data = date.lat.long, keep=direction, tz=timezone)$sunset } else if(direction == &quot;sunrise&quot;){ getSunlightTimes(data = date.lat.long, keep=direction, tz=timezone)$sunrise } else if (direction == &quot;dusk&quot;){ getSunlightTimes(data = date.lat.long, keep=direction, tz=timezone)$dusk } else if (direction == &quot;dawn&quot;){ getSunlightTimes(data = date.lat.long, keep=direction, tz=timezone)$dawn } } time_to_minute is the helper function the Stanford Open Policing Project uses in their tutorial to help convert times into a numeric values that’s easier to manipulate - this will be useful when splicing out times between sunset and dusk to remove ambiguouity in the intertwilight zone. input: time (character) output: minutes (double) time_to_minute &lt;- function(time) { hour(hms(time)) * 60 + minute(hms(time)) } add_night_day function utilizes oursunriseset function to mutate sunrise and sunset times and a binary variables to see if the stop happened in the night of day. In addition, this function takes out the intertwilightzone i.e. stops between sunset and dusk and dawn and sunrise. add_night_day &lt;- function(city_df, time_zone, lat, long){ sunset_times &lt;- city_df %&gt;% distinct(date) %&gt;% mutate(date = as.Date(ymd(date, tz = time_zone))) %&gt;% mutate(sunset = oursunriseset(lat, long, date, time_zone, direction=&quot;sunset&quot;), dusk = oursunriseset(lat, long, date, time_zone, direction=&quot;dusk&quot;), dawn = oursunriseset(lat, long, date, time_zone, direction=&quot;dawn&quot;), sunrise = oursunriseset(lat, long, date, time_zone, direction=&quot;sunrise&quot;), sunset = format(sunset, &quot;%H:%M:%S&quot;), dusk = format(dusk, &quot;%H:%M:%S&quot;), dawn = format(dawn, &quot;%H:%M:%S&quot;), sunrise = format(sunrise, &quot;%H:%M:%S&quot;), sunset_min=time_to_minute(sunset), dusk_min=time_to_minute(dusk), dawn_min=time_to_minute(dawn), sunrise_min=time_to_minute(sunrise)) %&gt;% drop_na() city_df &lt;- city_df %&gt;% drop_na() %&gt;% mutate(date=as.Date(ymd(date, tz = time_zone))) %&gt;% left_join(sunset_times, by=&quot;date&quot;) %&gt;% drop_na() %&gt;% mutate(minute = time_to_minute(time), minutes_after_dark = minute - dusk_min, is_dark = as.factor(minute &gt; dusk_min | minute &lt; dawn_min)) %&gt;% # filter out amiguous time between sunset and dusk and dawn and sunrise filter(!(minute &gt; sunset_min &amp; minute &lt; dusk_min), !(minute &lt; sunrise_min &amp; minute &gt; dawn_min)) %&gt;% select(subject_race, is_dark) # select(subject_race, search_conducted, subject_age, is_dark) return(city_df) } 3.3 Nationwide Functions These are functions used when analyzing multiple datatables from the database. relevant_datasets: input: - all dataset names which can be acquired through SHOW TABLES SQL command - A vector containing string of variables names we wish to dissect output: a character string of dataset names relevant_datasets &lt;- function(all_dataset_names, variables_of_interest){ # create empty vector datasets_of_interest &lt;- c() for(city in all_dataset_names){ # run only if table is not empty check_command = paste(&quot;SELECT 1 FROM&quot;, city, sep=&quot; &quot;, &quot;LIMIT 1&quot;) count &lt;- DBI::dbGetQuery(con, check_command) if(nrow(count) != 0){ # cancenate SQL query string command &lt;- paste(&quot;EXPLAIN&quot;, city, sep = &quot; &quot;) field_vector &lt;- unlist(as.list(DBI::dbGetQuery(con, command))$Field, use.names = FALSE) # of_interest_book is TRUE iff field_vector contains all the variables of interest of_interest_bool &lt;- setequal(intersect(field_vector, variables_of_interest), variables_of_interest) # add dataset name to vector if of_interest_bool if(of_interest_bool){ datasets_of_interest &lt;- c(datasets_of_interest, city) } } } return(datasets_of_interest) } query_data: input: name (character) output: dataframe Note: the command variable must be modified to the variables you are examining fix_ages quickly sets any ages to a dbl data type fix_ages &lt;- function(city_dataset){ city_dataset &lt;- city_dataset %&gt;% mutate(subject_age = as.numeric(subject_age)) return(city_dataset) } logistic_regression input: - city dataset (dataframe) - name of city (character) output: a dataframe where the columns are the coefficients (only returns one row of coefficient matrix; this function is insteaded to be used in a for loop or mapply so that we run the logistic regression on every dataset) coefficient_matrix &lt;- data.frame(&quot;intercept&quot; = numeric(), &quot;subject_age&quot; = numeric(), &quot;subject_race&quot; = as.numeric(), &quot;subject_age.subject_race&quot; = as.numeric(), &quot;dataset_name&quot; = character()) logistic_regression &lt;- function(city_dataset, name){ # run logistic regression fitlog &lt;- glm(formula = search_conducted ~ subject_race*subject_age, data = city_dataset, family = binomial, control = list(maxit = 50)) # record logistic regression coefficients coefficient_row_vector = t(fitlog$coefficients) # row bind each coefficient and dataset tibble with the coefficent_matrix coefficient_matrix &lt;&lt;- rbind(coefficient_matrix, cbind.data.frame(as.data.frame(coefficient_row_vector), name)) } "],
["description-of-data.html", "Chapter 4 Description of Data 4.1 Querying Datasets with RMySQL 4.2 Web Scrapping and Uploading Datasets", " Chapter 4 Description of Data The data was extracted from the Stanford Open Policing Project data hub. The Open Policing Project’s website contains ninety city and state-wide standardized stop datasets in both RDS and CSV format. From there, we uploaded six datasets to a mySQL database and locally queried the data on our personal laptops via RMySQL package within R. The possible variables within each dataset are listed below with description and corresponding units. Variable Description Units Stop Date States the year, month, day XXXX-XX-XX Stop Time States the time of the stop XX:XX:XX Stop Location States the street intersection of the stop Names two streets, occasionally lists a city Driver Race Describes the race of driver as given on the driver’s license String representing the subject’s race Driver Sex Describes the sex orientation of driver Binary Female/Male Driver Age Describes the age of the driver Integer representing the driver’s age Search Conducted Describes whether there was a search conducted or not True (search conducted) or False (search not conducted) Contraband Found Describes whether there was contraband found after a search was conducted True (contraband found) or False (contraband not found) Citation Issued Describes whether there was a citation issued to the driver True (citation issued) or False (no citation issued) Warning Issued Describes whether there was a warning issued to the driver True (warning issued) or False (no warning issued) Frisk Performed Describes whether there was a frisk performed on the driver True (frisk performed) or False (no frisk performed) Arrest Made Describes whether the driver was arrested True (arrest made) or False (no frisk performed) Reason for Stop Describes why the driver was stopped String describing the reason for the stop Violation Describes the violation when a citation or warning was issued String representing the violation We focused on the datasets for San Francisco, San Diego, Oakland, Raleigh, Charlotte, and Durham, and San Antonio. Below, the “X” represents whether the dataset for that particular city includes the specified variable. City Number.of.Observations Stop.Date Stop.Time Stop.Location Driver.Race Search.Conducted Contraband.found Citation.Issued Warning.Issued Frisk.Performed Arrest.Made Reason.For.Stop Violation San Francisco 905,070 X X X X X X X X X X Oakland 133,405 X X X X X X X X X San Diego 382,844 X X X X X X X X X X Charlotte 1,598,453 X X X X X X X X X X Durham 326,024 X X X X X X X X X X Raleigh 856,400 X X X X X X X X X X San Antonio 1,040,428 X X X X X X X X X While many of the datasets included a multitude of the variables above, there were clear limitations. First, while the California datasets were rich in variables, the “reason for stop” was dubious at best. Not only did many reasons make no logical sense, but also up to four reasons were listed for a single observation. Moreover, there is no uniformity in the “reason for stop” for all datasets in California. In contrast, the datasets in North Carolina had a clean “reason for stop”; however, these datasets do not have latitude/longitude which limits the geographical comparisons we can make. Additionally, only a small fraction of the city and state datasets contained subject age; the scarcity of information regarding subject age makes it difficult to compare nationally. Lastly, only a few datasets, such as Chicago, contained the police officer’s identification number. While we did not investigate the effect of the age, race, or other factors of individual police officers, it would be meaningful for more datasets to contain that information to draw national correlations. 4.1 Querying Datasets with RMySQL First import the necesarry packages to work with SQL in R library(XML) library(RMySQL) ## Loading required package: DBI library(tidyverse) library(knitr) library(rlist) ## Warning: package &#39;rlist&#39; was built under R version 3.6.3 Next, setup a connection to the database. # Connect to database con &lt;- dbConnect( MySQL(), host = &quot;traffic.st47s.com&quot;, user = &quot;student&quot;, password = &quot;Sagehen47&quot;, dbname = &quot;traffic&quot;) Note: If you are looking to modify the database you must use a different username and password that can be acquired through Professor Hardin. dbGetQuery returns a dataframe, so storing it in a variable adds that dataframe to the global environment raleigh_df &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM NCraleigh LIMIT 15&quot;) 4.1.1 Querying Subsets of dataset Certain scenarios such as running a logistic regression need not every variable from a dataset - only the variables of interest. Moreover, running a logistic regression on every city with all columns is a taxing operation. This is where querying can be useful in selecting portions of the datasets deemed essential. The logistic regression called for the race, age, sex, and data variables. The query_data function will takes in a database table name (city_name) and will select the desired variables from that table. query_data &lt;- function(city_name){ # cancenate SQL query string command &lt;- paste(&quot;SELECT subject_age, subject_race, subject_sex, date, search_conducted FROM&quot;, city_name, sep = &quot; &quot;) return(DBI::dbGetQuery(con, command)) } # Make a list of datasets datasets &lt;- list() datasets &lt;- lapply(datasets_of_interest, query_data) 4.2 Web Scrapping and Uploading Datasets This section will show the dataset upload process. The first step is to acquire all the RDS weblinks from the Stanford Policing Project Website. The code here is written by Professor Sarkis and it will get all the rds files from the stanford policing project website. Then, it stores all the file names into a character string url &lt;- &quot;https://openpolicing.stanford.edu/data/&quot; doc &lt;- htmlParse(readLines(url), asText=TRUE) links &lt;- xpathSApply(doc, &quot;//a/@href&quot;) free(doc) all_links &lt;- as.character(links[grep(&#39;_.*rds&#39;, links)]) One could also get all the dataset file names for a specific state by adding the state’s two character abbreviation e.g. california_links &lt;- as.character(links[grep(&#39;ca_.*rds&#39;, links)]) Another optional step is to splice certain links using base R indexing # subset list to get desired links all_links &lt;- all_links[43:48] all_links &lt;- all_links[-c(1, 2, 5)] Since the datasets are written to the database using the dbWriteTable function from RMySQL library, logical datatypes get interpreted as character, which will be tedious to deal with in future analysis. Therefore, the convertLogicalToInt function will convert columns that have logical datatypes into doubles (0 and 1). convertLogicalToInt function: input: dataset output: dataset with logical values set to binary convertLogicalToInt &lt;- function(input_df){ vars &lt;- sapply(input_df, class) %&gt;% list.which(.==&#39;logical&#39;) input_df[,vars] &lt;- sapply(input_df[,vars], as.numeric) input_df return(input_df) } Subsequently, dbWriteTable also needs a name for the table, which can parsed out of the weblinks. The getDFName function returns a suitable datatable name. getDFName: input: name of dataset (this could also be name of link from web scrape) output: clean dataset name that includes state and city getDFName &lt;- function(input_string){ tmp &lt;- str_split(input_string, &#39;_&#39;, simplify = TRUE) tmp state &lt;- str_to_upper(tmp[,2]) if(tmp[,4] != &quot;2019&quot;){ cityName &lt;- paste(tmp[,3], tmp[,4], sep=&quot;&quot;) name &lt;- paste(state, cityName, sep=&quot;&quot;) } else { cityName &lt;- tmp[,3] name &lt;- paste(state, cityName, sep=&quot;&quot;) } name return(name) } The penultimate step is to read the RDS file and write that to the database using the dbWriteTable function. The results from the two previous functions will serve as arguments for the dbWriteTable function. uploadLinksToDatabase: input: string link from web scrape outout: 0 - indicating that the code finished executing This function will take in a RDS link and write the dataset to the database uploadLinksToDatabase &lt;- function(input_link){ tmpDF &lt;- readRDS(gzcon(url(input_link))) tmpDF &lt;- convertLogicalToInt(tmpDF) name&lt;- getDFName(input_link) dbWriteTable(con2, name, tmpDF, overwrite = TRUE) return(0) } lapply serves as a ‘oneline’ for loop. The argument all_links is a character string of links, which lapply will iterate through and call the function, uploadLinksToDatabase, on each link lapply(all_links, uploadLinksToDatabase) "],
["exploratory-data-analysis.html", "Chapter 5 Exploratory Data Analysis 5.1 Demographics of Motorists Stopped, Nationwide 5.2 Dataset-specific Exploration 5.3 Modeling the Probability of Seach Given Stop", " Chapter 5 Exploratory Data Analysis 5.1 Demographics of Motorists Stopped, Nationwide We begin our exploratory data analysis by examining the recorded race, age, and sex of motorists involved in traffic stops. The goal of this section is to provide a high-level view of our 75 datasets. In particular, we see similarities and differences among states or cities. One trend is the consistently higher relative proportions of younger Black and Hispanic drivers than younger white drivers in their respective racial group. One notable variation is the police search rate: the white search rate in New Jersey is more than 75%, while the search rate for most other states and races is less than 25%. From these simple descriptive statistics, we pose further questions for the subsequent analyses of individual datasets in the next section. A concurrent goal of this section is to sense the potential (and limitations) of working with our data. While querying one dataset to generate a dataset-level plot is feasible, I use SQL commands SELECT, COUNT, and GROUP BY to circumvent the memory problem posed by generating a national-level plot. Furthermore, the ubiquitous baseline problem (the lack of traffic data to serve as a denominator for our traffic stop data) crops up when interpreting these initial visualizations. Lastly, we gauge the comprehensiveness of our data. Missing data within individual datasets and missing variables within our datasets limit the size of our observations. Thus, this exercise provides a rough scope of what can and cannot be done with our data and foregrounds our remaining analyses. 5.1.1 Set Up I first load the following packages. The package geofacet enables nationwide plots to be displayed in a map-like manner. I also connect to our SQL server, but that code is excluded here because of privacy. library(tidyverse) library(RMySQL) library(stringr) library(ggplot2) library(geofacet) ## Warning: package &#39;geofacet&#39; was built under R version 3.6.3 I query the entire Oakland dataset. Before creating each nationwide visualization, I first use the Oakland dataset to create a local-level version. I include the intermediate Oakland visualizations in this section to illustrate the different methods of preparing city-level versus nation-level plots. Also, the intermediate plot adds some coherency to the interpretation and significance of the nationwide plot. CAoak &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM CAoakland&quot;) 5.1.2 Race and sex of stopped motorists This visualization will show the breakdown, by race and sex, of motorists who were stopped in a given dataset. I convert the counts of motorists in each demographic category to be in percentages, which conveys the relative size of each demographic group to the rest of the drivers stopped. (Crucially, this percentage does not compare the size of the demographic group to all drivers.) The first version relays information from just one data set, Oakland, and the second version makes use of all data sets that record race and sex. Neither visualization conveys the extent of missing information; that is, I ignore the traffic stops in which race and/or sex are missing under the assumption that patrol officers fail to record data uniformly across motorists of different demographic groups. 5.1.2.1 Race and sex of stopped motorists in Oakland For the Oakland visualization, I use dplyr functions to calculate the percent of stops in each racial group. Then, I plot the data in a bar chart. CAoak %&gt;% # remove missing data filter(!is.na(subject_sex) &amp; !is.na(subject_race)) %&gt;% # use group_by and summarize to count number of stops per category group_by(subject_sex, subject_race) %&gt;% summarize(count = n()) %&gt;% ungroup() %&gt;% # find the percentage of age/race stops mutate(percentage = round(prop.table(count), digits = 2)) %&gt;% # plot percentages ggplot(mapping = aes(x = subject_sex, y = percentage, fill = subject_race, label = scales::percent(percentage))) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + # adjust labels geom_text(position = position_dodge(width = .9), vjust = -0.5, size = 3) + scale_y_continuous(labels = scales::percent) We observe that within each racial group, the percentage of stopped males is double that for females. We cannot conclude, though, that a Black male driver is twice as likely than a Black female drive to be stopped. Additionally, we cannot conclude anything about over- or under-representation of racial groups in the stop data. Although we can concretely point out that 7% of stopped drivers in Oakland are white males, we do not possess if that 7% should be higher or lower in the absence of discriminatory traffic patrol behavior. 5.1.2.2 Race and sex of stopped motorists, nationally The national level plot contains information only for races white, Black, and Hispanic for greater ease of interpretation on the larger plot. To create a national version of the age race bar graph, I do the following: 5.1.2.3 1. Create a list of relevant datasets I use the function relevant_datasets to find the datasets that have variables subject_sex and subject_race. # create list of all dataset names all_dataset_names &lt;-as.list(DBI::dbGetQuery(con, &quot;SHOW TABLES&quot;))$Tables_in_traffic demographics_variables &lt;- c(&quot;subject_race&quot;, &quot;subject_sex&quot;) race_sex_datasets &lt;- relevant_datasets(all_dataset_names, demographics_variables) 5.1.2.4 2. Query and clean data With the relevant data sets for the nationwide demographic visualization, I can run SQL queries on those data sets. I write the function query_count_RaceSex that takes as input a data set name and returns a cleaned dataframe with the percentage of each racial group calculated from a 50% random sample of each data set. query_count_RaceSex &lt;- function(dataset_name){ dataset_str = paste(dataset_name) # take a 50% random sample of each dataset for speed command &lt;- paste(&quot;SELECT subject_race, subject_sex, COUNT(*) as &#39;stops&#39; FROM&quot;, dataset_str, &quot;WHERE rand() &lt;= .5 GROUP BY subject_race, subject_sex&quot;, sep = &quot; &quot;) # query the data demographics_dataset &lt;- dbGetQuery(con, command) if(dim(demographics_dataset)[1] &lt; 1){ # disregard empty dataset return(NULL) } # clean dataset a bit demographics_dataset &lt;- demographics_dataset %&gt;% # use substr(), later on, to index into string and denote dataset mutate(dataset = paste(dataset_str), # make subject_age to be type double stop_percent = prop.table(stops)) return(demographics_dataset) } Using base R’s lapply function, I create a list of data frames with variables: race, sex, percent of stops, and data set name. race_sex_list &lt;- lapply(race_sex_datasets, query_count_RaceSex) 5.1.2.5 3. Combine and plot I combine the list of data frames – each row of this combined data frame has the percentage of motorists stopped in each demographic group. Next, I filter the data to include only demographic groups that are a combination of male and female with white, Black, and Hispanic. Through this, I also exclude percentage observations for which sex and/or race is not recorded. I limit my visualization to only include three races for ease of interpretation. # filter out missing data and certain races race_sex_all &lt;- bind_rows(race_sex_list, .id = &quot;column_label&quot;) %&gt;% filter(subject_sex == &quot;female&quot; | subject_sex == &quot;male&quot;) %&gt;% filter(subject_race == &quot;white&quot; | subject_race == &quot;hispanic&quot; | subject_race == &quot;black&quot;) # plot of race and sex distribution race_sex_all %&gt;% ggplot(mapping = aes(x = subject_sex, y = round(stop_percent, digits = 2), fill = subject_race, label = scales::percent(round(stop_percent, digits = 2)))) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + # add annotations geom_text(position = position_dodge(width = .9), vjust = -0.5, size = 2) + scale_y_continuous(labels = scales::percent) + facet_wrap(~ dataset) + ylab(&quot;percentage&quot;) ggsave(&quot;race and sex distribution.png&quot;, width = 11, height = 9, units = &quot;in&quot;) Observations: There are more male drivers stopped than female drivers stopped across near all racial groups and data sets. While the reason for this trend is outside the scope of this report, this trend informs our future analyses to consider disaggregating stop or search behavior by sex. Indeed, this is what Rosenfeld et al. do – they examine search probabilities of only stops involving male drivers. There is no clear unifying trend of the racial distributions among the data sets. This reflects the range of variables that determine racial breakdown of stopped drivers – residential population, driving population, driving behavior, and traffic behavior are just a few. This plot gives us a broader sense of how finding evidence of discrimination is difficult. We do not know what the racial percentage of traffic stops should be; without a comparison, we cannot conclude is a population is over- or under-represented. Data collection is far from uniform. We observe that for data sets like MSstatewide and KYowensboro, there are no Hispanic drivers stopped; perhaps officers recorded motorists as “other.” 5.1.3 Distribution of motorists’ age by race This next visualization shows the density of motorist ages, by race, who were stopped in a given data set. For ease of interpretation, I only plot age densities for motorists recorded as white, Black, and Hispanic. To calculate the kernel density of each age group, I use ggplot’s geom_density function. Geom_density is a smoothed version of a histogram; the percentage of stopped drivers in a given racial group that is in a given age range is relayed by the area of the curve for that age range. As in the race and sex visualization, the percentages (or kernel estimates) for each age group do not relate drivers stopped at a certain age with all drivers in the area. Instead, the densities relate drivers stopped at a certain age with other drivers stopped. The first visualization relays information from just one data set, Oakland, and the second version makes use of all data sets that record subject age and race. Neither visualizations relay the extent of missing data, making use only of the observations that have age and race recorded. Also, I limit the range of subject ages to be less than 80. 5.1.3.1 Age distribution by race in Oakland For the Oakland plot, I simply pipe the filtered Oakland data into ggplot. I let transparency, denoted by alpha, equal .5 to more easily discern trends in the graph. CAoak %&gt;% filter(subject_age &lt; 80) %&gt;% filter(subject_race != &quot;other&quot; &amp; subject_race != &quot;asian/pacific islander&quot;) %&gt;% ggplot + geom_density(mapping = aes(x = subject_age, fill = subject_race, color = subject_race), alpha = .5) Observations: The densities of people of color (POC) drivers stopped is more skewed right than that for white drivers, with greater weights at younger ages. The densities of POC drivers decline faster between ages 20-40 than that for white drivers. The relative weights of POC drivers age 30 and older is, in fact, lower than that for white drivers that age. A note on interpretation: again, the denominator of the kernel estimates is all other drivers who were stopped. This graph doesn’t say that younger POC drivers are more likely to be stopped than younger white drivers. Rather, of the drivers stopped, younger POC drivers are represented more frequently than same-age white drivers. 5.1.4 Age distribution by race, nationally The national level plot contains information only for races white, Black, and Hispanic and ages less than 80. To create a national version of this age density plot, I do the following: 5.1.4.1 1. Create a list of relevant datasets I use the function relevant_data sets to find the data sets that have variables subject_age and subject_race. # all_dataset_names is a list of all data set names, already exists earlier in the code age_variables &lt;- c(&quot;subject_age&quot;, &quot;subject_race&quot;) age_datasets &lt;- relevant_datasets(all_dataset_names, age_variables) 5.1.4.2 2. Query and clean data With the relevant data sets for plotting age distribution, I write a function query_clean_AgeRace. As input, the function takes a data set name and returns a 50% random subset of the data frame; I only query the age and race columns for efficiency and memory reasons. Thus, I also limit my subsetted data for races white, Black, and Hispanic and for motorists under the age of 80. (Originally, I planned to use the COUNT and GROUP BY commands in SQL but could not find a way for geom_density to take the number of stops as inputs.) query_clean_AgeRace &lt;- function(dataset_name){ dataset_str = paste(dataset_name) # introduce cap of age 80 for motorists b/c querying big data leads to problems # take a 50% sample of each dataset command &lt;- paste(&quot;SELECT subject_age, subject_race FROM&quot;, dataset_str, &quot;WHERE (subject_race = &#39;black&#39; OR subject_race = &#39;white&#39; OR subject_race = &#39;hispanic&#39;) AND subject_age &gt; 0 AND subject_age &lt; 80 AND rand() &lt;= .5&quot;, sep = &quot; &quot;) age_dataset &lt;- dbGetQuery(con, command) if(dim(age_dataset)[1] &lt; 1){ # disregard empty dataset return(NULL) } # clean dataset a bit age_dataset &lt;- age_dataset %&gt;% # use substr(), later on, to index into string and denote dataset mutate(dataset = paste(dataset_str), # make subject_age to be type double subject_age = as.double(subject_age)) return(age_dataset) } Using the lapply function, I create a list of data frames with variables: race, age, and data set name. Each row in these data frames contains information on one traffic stop from a data set. age_race_list &lt;- lapply(age_datasets, query_clean_AgeRace) 5.1.4.3 3. Combine and make intermediate plot I combine the list of data frames from the previous step. I also create an intermediate plot that does not use geofacet but simply facets by the data set, so city and statewide data sets are plotted separately. age_race_all &lt;- bind_rows(age_race_list, .id = &quot;column_label&quot;) # plot of racial age distributions per dataset age_race_all %&gt;% ggplot() + geom_density(aes(x = subject_age, color = subject_race), alpha = .3) + facet_wrap(~ dataset) ggsave(&quot;racial age distribution without fill.png&quot;, width = 11, height = 9, units = &quot;in&quot;) Observations: For the current racial age distribution plot, the blue lines representing white drivers reach their maximum density level that is usually almost always less than the maximum densities of red and green lines, representing Black and Hispanic drivers. As in the Oakland-level plot, we thus see how across data sets, the densities of people of color drivers stopped is more skewed right than that for white drivers, with greater weights at younger ages. The densities of POC drivers usually decline faster between ages 20-40 than that for white drivers. That is, the slope for POC drivers is usually steeper than for white drivers between those ages. The blue lines seem to peak around age 25 to 30 before decreasing, only to plateau at between ages 40 and 50. The relative weights of POC drivers age 30 and older is, in fact, lower than that for white drivers that age. By examining the data sets together, we can discern how the age densities of Oakland are not exactly replicated in other places. When we aggregate some data data sets to plot by state, some of the granularity among intra-state age distributions will be lost. Also, by examining the data sets together, we are reminded of uneven data collection. Iowa statewide (IAstatewide) only has a green line because subject_race is recorded most frequently as NA, then “other.” I checked Iowa’s data set and found that stops with race recorded as Hispanic make up less than 2.5% of the total stops recorded. Also, the spikes in the North Carolina data sets is probably indicative of patrol practices of estimating ages of stopped motorists. One possible future direction would be to plot the data into two separate plots, one for male and one for female. 5.1.4.4 4. Plot using geofacet The next part of cleaning is extracting state name using stringr. By extracting the state abbreviation from the “dataset” variable, I can later pipe the data into geofacet to create a map-like plot. age_race_all &lt;- age_race_all %&gt;% # create state and city variables using substrings mutate(state = substr(dataset, start = 1, stop = 2), # city variable will be &quot;statewide&quot; if data set isn&#39;t city level city = ifelse(str_detect(dataset, &quot;statewide&quot;), state, str_extract(substr(dataset, start = 3, stop = nchar(dataset)), &quot;[a-z]+&quot;))) Because I extract the state abbreviation from the “dataset” variable, I can now pipe the data into ggplot, using geofacet to create a map-like plot. ggplot(data = age_race_all, aes(x = subject_age, color = subject_race)) + geom_density(alpha = .3) + facet_geo(~ state) + theme_bw() ggsave(&quot;geofacet racial age distribution.png&quot;, width = 11, height = 9, units = &quot;in&quot;) Observations: We do lose some granularity from the differences in patrol behavior per state, but the peaks and slope observations from the previous two plots still hold here. For brevity, I include them both here: the densities of people of color drivers stopped is more skewed right than that for white drivers, with greater weights at younger ages. Furthermore, the densities of POC drivers usually decline faster between ages 20-40 than that for white drivers. By placing the plots in geographic relation to one another, the extent of uneven data collection becomes clearer. Only 45 out of our total 75 data sets have the two variables necessary to generate this plot. 5.1.5 Search rates For this last nationwide visualization, we examine the search rates of different states, disaggregated by race. The search rate is calculated by taking the total number of searches and dividing by the total number of stops; it represents a conditional probability of being searched given one is stopped. 5.1.6 1. Query the data We use the conveniently formatted datasets opp_stops_state and opp_stops_city. The two data sets have the following variables: search_rates, stops_rates, and subject_race for every state. opp_stops_state &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM opp_stops_state&quot;) opp_stops_city &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM opp_stops_city&quot;) 5.1.7 2. Transform data We then combine certain observations from the two data sets to maximize our state coverage. Next, to find the average stop rate per racial group and state, I use tidyverse functions group_by (by race and state) and summarise; then, I average the stop rates. As I am taking the simple average, this average stop rate does not reflect the different number of stops per data set in a given state. For example, in a given state, say that there are two police departments, one that conducts 1 million stops total and one that conducts 1,000 stops total. In my average search rate, the search rates of both these hypothetical police department is weighted equally. # retrieve additional states from the opp_stops_city dataset opp_stops_state &lt;- rbind(opp_stops_state, opp_stops_city %&gt;% filter(state==&quot;NJ&quot; | state==&quot;OK&quot; | state==&quot;PA&quot; | state==&quot;KS&quot; | state==&quot;KY&quot; | state==&quot;LA&quot; | state==&quot;MN&quot;)) %&gt;% # We only want to look at states, race, and stop_rate; this could be # modified to display other variables select(&quot;state&quot;, &quot;subject_race&quot;, &quot;stop_rate&quot;) %&gt;% # find average stop rate per racial group per state group_by(subject_race, state) %&gt;% summarise(stop_rate = mean(stop_rate)) 5.1.8 3. Plot using geofacet The first nationwide plot is a bar chart, faceted by states. observations: The main takeaway from this bar plot is that the search propensity of police departments (aggregated into states) varies widely. States like New Jersey, Pennsylvania, and Montana are the only states with search rates for a racial group that reach 25%. The majority of states have average search rates under 25%; for these states, this chart is not particularly useful for seeing trends because of axis scaling. Presented with these search rates, we are again confronted with the difficulty of proving discrimination. Looking at search rates by race alone does not provide convincing evidence of or against racial profiling. 5.1.9 4. Use scatter plot to examine ratios The second nationwide plot makes use of scatter plots and relays the ratio of search rates between Black and white and Hispanic and white search rates. I use color coding for the ratio of the search rate for a POC group to the corresponding white search rate. Each dot in the following graphs represents one state. I include a y = x line to indicate where the ratios would be, if they were equal. opp_stops_state %&gt;% ungroup(subject_race) %&gt;% spread(key = subject_race, value = stop_rate) %&gt;% ggplot(aes(x = white, y = black, color = black/white)) + geom_point() + geom_abline(slope=1, intercept=0) + xlab(&quot;white search rate&quot;) + ylab(&quot;Black search rate&quot;) + scale_colour_gradient(low = &quot;#56B1F7&quot;, high = &quot;#FA5858&quot;, limits = c(0, 3.5)) opp_stops_state %&gt;% ungroup(subject_race) %&gt;% spread(key = subject_race, value = stop_rate) %&gt;% ggplot(aes(x = white, y = hispanic, color = hispanic/white)) + geom_point() + geom_abline(slope=1, intercept=0) + xlab(&quot;white search rate&quot;) + ylab(&quot;Hispanic search rate&quot;) + scale_colour_gradient(low = &quot;#56B1F7&quot;, high = &quot;#FA5858&quot;, limits = c(0, 3.5)) Observations: The recorded Black-white search rate per state is generally greater than one, while the recorded Hispanic-white search rate is generally less than one. Reasons for these discrepancies range from poor data collection to discrimination, but which factors are driving this difference is far outside the scope of this plot. 5.2 Dataset-specific Exploration 5.2.1 Stop outcomes from day to night in Oakland, CA This section of data analysis incorporates our day variable with stop outcomes in the Oakland data set. 5.2.1.1 Set up I use the following packages for this analysis. Suncalc, lutz (a cute acronym for “look up time zone”), and lubridate are necessary for calculating sunset and sunrise times. I also connect to our SQL server, but that code is excluded here because of privacy. library(tidyverse) library(RMySQL) library(suncalc) library(lutz) ## Warning: package &#39;lutz&#39; was built under R version 3.6.3 library(lubridate) Next, I query the entire Oakland data set to be available locally. I also clean the data by removing the entries that do not record data, parsing my date and time variables with lubridate, and creating a POSIX type time variable that will serve as an input for suncalc functions. CAoak &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM CAoakland&quot;) # Lubridate CAoak &lt;- CAoak %&gt;% filter(!is.na(date)) %&gt;% mutate(nice_date = ymd(date), nice_year = year(nice_date), nice_month = month(nice_date), nice_day = day(nice_date), nice_time = hms(time), nice_day_of_year = yday(date), #for sunset sunrise: posix_date_time = as.POSIXct(paste(nice_date, time), tz = &quot;America/Los_Angeles&quot;, format = &quot;%Y-%m-%d %H:%M:%OS&quot;)) ## Warning in .parse_hms(..., order = &quot;HMS&quot;, quiet = quiet): Some strings failed to ## parse, or all strings are NAs 5.2.1.2 Set up sunset/sunrise times in CA Oakland Next, I use the function we wrote, oursunriseset, and dplyr functions to create a new set of variables in the Oakland data. For each stop, we analyze the time of day and day of the year to categorize the stop as either occurring during the day (for which the new variable light would be “day”) or during the night (for which light would be “night.” When I use the suncalc function getSunlightTimes, I let the input timezone be the Oakland timezone. OaklandTZ &lt;- lutz::tz_lookup_coords(37.75241810000001, -122.18087990000001, warn = F) oursunriseset &lt;- function(latitude, longitude, date, direction = c(&quot;sunrise&quot;, &quot;sunset&quot;)) { date.lat.long &lt;- data.frame(date = date, lat = latitude, lon = longitude) if(direction == &quot;sunrise&quot;){ getSunlightTimes(data = date.lat.long, keep=direction, tz = OaklandTZ)$sunrise }else{ getSunlightTimes(data = date.lat.long, keep=direction, tz = OaklandTZ)$sunset } } Next, I run oursunriseset on the Oakland data set, using mutate to create the intermediate variables, sunrise and sunset, and the light variable. I remove the traffic stop observations for which time was not recorded. # add light variable CAoak &lt;- CAoak %&gt;% # use oursunriseset function to return posixct format sunrise and sunset times mutate(sunrise = oursunriseset(lat, lng, nice_date, direction = &quot;sunrise&quot;), sunset = oursunriseset(lat, lng, nice_date, direction = &quot;sunset&quot;)) %&gt;% mutate(light = ifelse(posix_date_time &gt; sunrise &amp; posix_date_time &lt; sunset, &quot;day&quot;, &quot;night&quot;)) %&gt;% # about 100 NA&#39;s to filter out filter(!is.na(light)) # knitr::kable((CAoak %&gt;% select(posix_date_time, subject_race, light) %&gt;% head())) This short excerpt of the transformed Oakland data set shows how the time of stop and light variable correspond to one another. 5.2.1.3 Stop outcomes, by race and day/night With the set up done, we can begin plotting stop outcomes related to day and night! 5.2.1.3.1 Counts, by race The bar chart shows the number of stops per race occurring during the day and night. # see how drivers are stopped by race and light in absolute counts ggplot(data = CAoak) + geom_bar(mapping = aes(x = subject_race, fill = light)) The bar chart relays effects of a slew of variables, such as driving behavior of a time of day, time of year, and racial group; traffic and patrol behavior for a given time and location is also relayed. Thus, again, we cannot make definitive statements about racial profiling. As in the veil of darkness assumption, the race of a motorist is harder to discern during the night than during the day. However, just because the day-night bars in each racial group looks to be about equal neither corroborate nor denies racial profiling. We are without information on the true traffic infringement rate per racial group in different times of the day. 5.2.1.4 Day/night outcomes of stop by race Next, I examine how outcome of stop, race, and day/night interact. I pipe the Oakland data set into ggplot, allowing the color of each bar to represent a traffic stop outcome and the transparency of the bars to denote day and night stops. # how are outcomes of stops affected by race and time of day? CAoak %&gt;% ggplot(aes(x = subject_race, fill = outcome, color = outcome, alpha = light)) + geom_bar(position = &quot;dodge&quot;) + scale_alpha_manual(values = c(.2, .9)) Observations: For each type of outcome and race, stops during the day outnumber stops during the night. This may result from the Oakland police department’s patrol patterns, driver behavior during the day, and/or another factor. Missing data regarding traffic outcome does not look to be evenly distributed among stopped motorists – missing data during the day and night look to be about the same for all racial groups except for Black. Citations look to be the most frequent outcome of traffic stops for all racial groups, followed by warning, then arrest (not including missing data.) Future directions could look more deeply into the probabilities of each of these outcomes for different racial groups, as the nature of a stricter outcome (being issued a citation rather than a warning) may not be uniformly distributed among racial groups. Furthermore, incorporating the light variable in this direction could be illuminating: are traffic patrols more lenient during the day or during the night once a motorist is stopped? Does the extent of this leniency differ for different racial groups? 5.2.2 Stop outcomes in San Diego, CA 5.2.2.1 Significance of Stop Outcomes It is helpful to look at the distribution of stop outcomes because it could suggest underlying intent from the officer or commonalities in policing different races (i.e. prevalence of profiled searches, warrant for arrest). Additionally, it can communicate trends in policing behavior in regards to race of a particular police department. The following visualizations display the distribution of searches and arrests made on traffic stops in San Diego. 5.2.2.2 Recoding Race column in San Diego Dataset Unlike many datasets, in San Diego, subject race is recorded in a very detailed and consistant manner. However, for the purpose of comparing with other datasets used in the Stanford Open Policing Project, the below race selections were recodded as \"ASIAN/PACIFIC ISLANDER\" to reflect how race is categorized by police departments in other cities and states. The recode function can be used to recode entries in the raw_subject_race_description column as such. Here the recode function is used to overwrite some raw_subject_race_description entries as \"ASIAN/PACIFIC ISLANDER\". goodSD$raw_subject_race_description &lt;- recode(goodSD$raw_subject_race_description, VIETNAMESE = &quot;ASIAN/PACIFIC ISLANDER&quot;, SAMOAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, LAOTIAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, KOREAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, JAPANESE = &quot;ASIAN/PACIFIC ISLANDER&quot;, INDIAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, HAWAIIAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, GUAMANIAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, FILIPINO = &quot;ASIAN/PACIFIC ISLANDER&quot;, CHINESE = &quot;ASIAN/PACIFIC ISLANDER&quot;, CAMBODIAN = &quot;ASIAN/PACIFIC ISLANDER&quot;, `ASIAN INDIAN` = &quot;ASIAN/PACIFIC ISLANDER&quot;, `OTHER ASIAN` = &quot;ASIAN/PACIFIC ISLANDER&quot;, `PACIFIC ISLANDER` = &quot;ASIAN/PACIFIC ISLANDER&quot;) 5.2.2.3 Plotting Search Conducted by Race Below is that code for creating a data table with generated percentages of search and non-search traffic stops in San Diego. The following steps are replicated for person searches, vehicle searches, arrests made, citations issued, and warnings issued. # create a new dataset SearchesByRaceCOL &lt;- goodSD %&gt;% # group by race and search_conducted # columns group_by(raw_subject_race_description, search_conducted) %&gt;% # calculate count for searches conducted # by race summarize(Total = n()) %&gt;% # group by race group group_by(raw_subject_race_description) %&gt;% # calculate percentage of searches # conducted mutate(pct = Total/sum(Total)) # save the percentages in data table data.table(SearchesByRaceCOL) ## raw_subject_race_description search_conducted Total pct ## 1: ASIAN/PACIFIC ISLANDER FALSE 31147 0.97203757 ## 2: ASIAN/PACIFIC ISLANDER TRUE 896 0.02796243 ## 3: BLACK FALSE 38217 0.90949548 ## 4: BLACK TRUE 3803 0.09050452 ## 5: HISPANIC FALSE 108932 0.94456536 ## 6: HISPANIC TRUE 6393 0.05543464 ## 7: OTHER FALSE 26420 0.98350892 ## 8: OTHER TRUE 443 0.01649108 ## 9: WHITE FALSE 155130 0.97223006 ## 10: WHITE TRUE 4431 0.02776994 ## 11: &lt;NA&gt; FALSE 6893 0.95537076 ## 12: &lt;NA&gt; TRUE 322 0.04462924 # omit NAs for searches conducted SearchesByRaceCOL &lt;- na.omit(SearchesByRaceCOL) Below is a bar plot of SearchesByRaceCOL which visualizes the percentage of traffic stops that resulted in a search binned by race group. It is important to note that search conducted includes all stops that result in a search of person and search of vehicle but are not composed of entirely of the two. The following two bar plots visualize search of person and search of vehicle percentages by race. 5.2.2.4 Plotting Search Person by Race 5.2.2.5 Plotting Search Vehicle by Race 5.2.2.6 Plotting Arrest Made by Race It is important to note that if an arrest was made, it does not necessarily mean that a traffic violation was the cause of arrest. By analyzing the reason for stop column, one can easily see that pedestrians and vehicles are included in the traffic stop data set. Additionally, the data set does not account for stops that were made because of a prexisting arrest warrant which dictates that race was not a motivation in making the arrest/stop. 5.2.2.7 Plotting Citations Issued by Race 5.2.2.8 Plotting Warnings Issued by Race From observing all possible stop outcomes and their proportions, it is observed in San Diego, arrest is not as prevalent in traffic stops. However, citations and warnings are much more prevalent proportionally with relatively equivalent percentages across all races. 5.2.3 Weather overlay in Raleigh, NC This section of exploratory data analysis sees if precipitation shows correlation with the amount of traffic stops per day. The expected correlation is that if it rains more there will be an increase in traffic stops because wet roads could lead to more reckless driving. The Rnoaa package requires an API key, which can be acquired from the Rnoaa documentation 5.2.3.1 Visualizing Weather in Raleigh, North Carolina In this example, we will overlay precipitation over traffic stops in Raleigh February. raleigh &lt;- DBI::dbGetQuery(con, &quot;SELECT * FROM NCraleigh&quot;) In order to get weather data we must call the ncdc function. The key argument to keep in mind is the stationid argument. In order to find a city’s station ID you must go to this NOAA’s website https://www.ncdc.noaa.gov/cdo-web/search. Moreover, the type of data such as precipation, temperature, and snow etc is contingent on the station ID. For this example, we will look at precipitation only, but there is room to explore more variables to see in any affect traffic. out &lt;- ncdc(datasetid = &quot;GHCND&quot;, stationid = &quot;GHCND:USC00317079&quot;, datatypeid = &quot;PRCP&quot;, startdate = &quot;2013-01-01&quot;, enddate = &quot;2013-12-31&quot;, limit = 500) Now that we have the two datasets we must clean them because we will be joining them. Here we use the lubridate package to make year, month, and day their own columns in the weather dataframe. weather_df &lt;- data.frame(out$data) # fix weather_df dates weather_df &lt;- weather_df %&gt;% mutate(clean_date = ymd_hms(date)) # make individual columns with year, # month, &amp; day weather_df &lt;- weather_df %&gt;% mutate(year = year(clean_date), month = month(clean_date), day = day(clean_date)) Similarly, the raleigh dataset must also have year, month, and day in separate columns. # use libridate to add clean date column # for lubridate to interpret raleigh &lt;- raleigh %&gt;% mutate(clean_date = ymd(date, tz = &quot;UTC&quot;)) # use lubridate to add year, month, day # columns raleigh &lt;- raleigh %&gt;% mutate(year = year(clean_date), month = month(clean_date), day = day(clean_date)) Now that we have the two dataframes cleaned up, we can follow a sequence of piping commands to join and plot them. # define month variable so we can easily # plot different months month_var = 3 title = paste(&quot;Stop Counts with Arrests and Precipitation Day in 2013-&quot;, paste(month_var)) # the pipe sequence below will examine a # month in a year such that we count all # the traffic stops that happened that # day. Then, we plot the stop counts for # a specific day and max temp in raleigh %&gt;% # filter out raleigh entries in year 2013 # and month variable filter(year == 2013, month == month_var, arrest_made == &quot;TRUE&quot;) %&gt;% # left_join by &#39;clean_date&#39; left_join(weather_df, by = c(&quot;clean_date&quot;)) %&gt;% # Group the data by day as this will be # our shared x-axis group_by(day.x) %&gt;% # count the number of stops per day and # the precipitation value of that day summarize(count = n(), precipitation = max(value)) %&gt;% ggplot() + geom_line(aes(x = day.x, y = precipitation, color = &quot;PRCP&quot;)) + geom_line(aes(x = day.x, y = count/0.05, color = &quot;Stop Count&quot;)) + scale_y_continuous(sec.axis = sec_axis(~. * 0.05, name = &quot;Stop Count&quot;)) + ggtitle(title) Based on the plot, it seems like precipitation in day does not impract the amount of stops. For example, the precipitation day 15 and 25 is relatively but stop count still fluctuates. Future analysis could look at the hourly level of stop counts and precipation or a year long look of precipiation and stop counts per day. 5.2.4 Maps in San Antonio, TX 5.2.4.1 Neccessary packages The following packages provide the tools to begin some geography analysis. Note that a SQL database was also used, but was not included in this code as it is demonstrated in previous sections. Please see https://cran.r-project.org/web/packages/RMySQL/index.html for further instructions in neccessary. library(lubridate) library(RMySQL) library(ggmap) library(leaflet) library(ggplot2) library(tidyverse) library(dplyr) 5.2.4.2 Geography Plots Our work surrounding Geography grew from a general curiosity about the latitude and longitude variables available on some datasets. The EDA on this topic began with a simple plot of the stops in San Antonio. My first step was querying the SQL database to create a local dataframe with the variables of interest. The variables were the following: longitude, latitude, subject race, subject sex, date of stop, and time of stop. I then cleaned the data to be in the appropriate units for my plot. Note that I include a random sample of the dataframe to lighten the load on the computer system to hopefully increase usability. # Create a new dataframe with variables # of interest coor &lt;- DBI::dbGetQuery(con, &quot;SELECT lng, lat, subject_race, subject_sex, date, time FROM TXsanantonio&quot;) # creates a label to be used for each # marker in cluster plot coor &lt;- coor %&gt;% unite(full_label, date, time, subject_race, subject_sex, sep = &quot;, &quot;, remove = FALSE) # changes columns to appropriate units # for plotting and color coding coor$lng &lt;- with(coor, as.numeric(lng)) coor$lat &lt;- with(coor, as.numeric(lat)) coor$subject_sex &lt;- with(coor, as.factor(subject_sex)) # create a random sample of 20000 to # decrease load on computer coor &lt;- sample_n(coor, 20000) To make my first simple map, I created a base map with longitude and latitude boundaries around San Antonio. Then, using the geom_point function, I plotted the stops over this base map color coding by subject race and varying shapes by subject sex. The grid for the base map can be changed for a hardcoded “zoom”. # creating the simple map with grid # latitude and longitude values map &lt;- get_map(c(left = -99, bottom = 29.2, right = -98, top = 29.9)) race_sex_plot &lt;- ggmap(map) + geom_point(aes(x = lng, y = lat, color = subject_race, shape = subject_sex), data = coor, size = 0.75) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + ggtitle(&quot;Simple Overlay Map of San Antonio Stops&quot;) + coord_quickmap() race_sex_plot The next step was to create interactive graphs. To do so, I used the leaflet package. Note that these will not show in the PDF version of this analysis. The first is a graph similar to the plot over a base map, but allows for zoom and movement. This graph is also color coded by subject race. #creating a color palette my_colors &lt;- colorFactor(&quot;plasma&quot;, domain = c(&quot;asain/pacific islander&quot;, &quot;black&quot;, &quot;white&quot;, &quot;hispanic&quot;, &quot;NA&quot;, &quot;other&quot;, &quot;unknown&quot;)) #creating the leaflet graph leaflet(coor) %&gt;% addProviderTiles(providers$Esri.NatGeoWorldMap) %&gt;% addCircleMarkers( color = ~my_colors(subject_race), # coloring by race stroke = FALSE, fillOpacity = 1, radius = 3) %&gt;% addLegend(pal = my_colors, values = ~subject_race, opacity = 1) The final graph is a cluster graph of all of the datapoints. Using the marketClusterOptions function, the graph automatically clusters the data, showing the number of stops in an area on the map. The area considered in each cluster is indicated by a blue marking when hovering the cursor over the number. Zooming in further will make these zones smaller. After zooming in enough to see individual stops, hovering over each marker will display a label of the date, time, race, and sex of the individual stopped. These labels can of couse be changed for different data sets. leaflet(coor) %&gt;% addTiles() %&gt;% addMarkers(clusterOptions = markerClusterOptions(), label = coor$full_label) 5.2.4.2.1 Observations, Questions, and Potential Extentions The investigation of Geography raised many questions and observations. The first observation was that there exist some latitude and longitude value that are not in San Antonio, but rather in Austin, Houston, and some closer to Louisiana. This may be a flaw in the dataset, but could also be that a San Antonio officer was in another city at the time of that stop. Neverless, it presents a potential area of concern for data usability. One question raised was the potential discrepancies between highway stops and stops in smaller towns for example. There may be differences in demographics for highway and non highway stops and understanding these differences is critical when trying to combat the issue of a denominator explained in the literature review. The cluster plot emphasized the importance of this distinction, with locations on highways having clusters of more than 200 stops for a single longitude and latitude. One many question if there are particular demographics travelling along these cluster locations and if so, how that may impact the stop counts. Another point of further research can be controlling for demographics of certain neighborhoods or districts and comparing stop rates to those demographics. This can be done a variety of ways and would of course require more data than can be found on the Stanford Open Policing Porject database. One may also look into any information on scheduling for police officers—where they are sent more often, at what points in the day, etc. 5.2.5 Socioeconomic factors in Durham, NC The reason behind a vehicle equipment stop seems to be an economical one, as drivers who have funds to fix their vehicles in the case of a broken tail light for example would be less likely to get stopped for an equipment violation. The plot is used to investigate this hypothesis, where, as median income increases, we look for the percentage of equipment violations to drop. # manual data entry incomeNC &lt;- data.frame(city = c(&quot;durham&quot;, &quot;raleigh&quot;, &quot;charlotte&quot;, &quot;fayetteville&quot;, &quot;greensboro&quot;), medianincome = c(58190, 63891, 60886, 44057, 46702), equipmentstops = c(0.1327, 0.0845, 0.0653, 0.1344, 0.1026), regulatorystops = c(0.2067, 0.248, 0.2166, 0.2632, 0.1758)) # plot for percent of equipment stops equipment &lt;- incomeNC %&gt;% ggplot() + geom_point(aes(x = medianincome, y = equipmentstops)) + geom_text(aes(x = medianincome, y = equipmentstops, label = city), hjust = 0.5, vjust = -0.5) + labs(title = &quot;Vehicle Equipment Stops vs Median Household Income&quot;) equipment This is a scatter plot of percentage of vehicle equipment stops against median household income (obtained from census.gov). The scatter plot reflects this relationship, however since there are so few observations, maybe more data is needed for a more comprehensive conclusion. # plot for percent of regulatory stops regulatory &lt;- incomeNC %&gt;% ggplot() + geom_point(aes(x = medianincome, y = regulatorystops)) + geom_text(aes(x = medianincome, y = regulatorystops, label = city), hjust = 0.5, vjust = -0.5) + labs(title = &quot;Vehicle Regulatory Stops vs Median Household Income&quot;) regulatory We also look at vehicle regulatory stops against median household income, which also seems to be related to economically implications. Here we see a more clear trend excluding Fayetteville, that there is an increase in percentage stops with the increase of median income. 5.3 Modeling the Probability of Seach Given Stop 5.3.1 Sunrise and Sunset During night times when the sun is set, researchers speculate that stops are made by police officers without the acknowledgment of the racial identity of the driver. This should be the exact opposite for police officers during the day times. With the hope of identifying possible underlying racial discrimination with the available stop data, it has therefore been determined that for a specific region, the differences between the percentage of minority drivers within day times and night times could be used to detect such discrimination. The central idea is that, by observing the distribution of the difference values through the progression of time, we can gain a better understanding of the change of police behaviors with the transformation from day times to night times. The calculation of the difference for this research project is set to be the daytime percentage of minority drivers (black) being stopped deducted by that of the night time percentage. A positive difference value indicates that if the police officer is able to identify the identity of the driver (i.e., under the day times), she or he has a higher percentage chance of stopping the minority driver. Whereas during the night times with a lack of sufficient light source, we assumed that the police officer was unable to identify the driver’s race. Therefore, if there exists racial discrimination against minority drivers, we should expect to observe that a significant proportion of percentage differences should be positive. However, if the visualization shows an opposite distribution, then we have to conclude that the division of day time and night time has failed to capture the existence of racial discrimination. SAN &lt;- SAN %&gt;% dplyr::mutate(date2 = lubridate::as_date(lubridate::ymd(as.character(date)))) SAN &lt;- SAN %&gt;% mutate(date_time = as.POSIXct(paste(date, time), tz = &quot;America/Chicago&quot;, format = &quot;%Y-%m-%d %H:%M:%OS&quot;)) oursunriseset &lt;- function(latitude, longitude, date, direction = c(&quot;sunrise&quot;, &quot;sunset&quot;)) { date.lat.long &lt;- data.frame(date = date, lat = latitude, lon = longitude) if (direction == &quot;sunrise&quot;) { x &lt;- getSunlightTimes(data = date.lat.long, keep = direction, tz = &quot;America/Chicago&quot;)$sunrise } else { x &lt;- getSunlightTimes(data = date.lat.long, keep = direction, tz = &quot;America/Chicago&quot;)$sunset } return(x) } sunrise &lt;- oursunriseset(29.4241, -98.4936, SAN$date2, direction = &quot;sunrise&quot;) sunset &lt;- oursunriseset(29.4241, -98.4936, SAN$date2, direction = &quot;sunset&quot;) SAN &lt;- cbind(SAN, sunrise, sunset) SAN &lt;- SAN %&gt;% mutate(light = ifelse(date_time &lt; sunrise, &quot;night&quot;, ifelse(date_time &gt; sunset, &quot;night&quot;, &quot;day&quot;))) racial_perc &lt;- function(mon, yr, race) { x1 &lt;- filter(SAN, light == &quot;day&quot;, month == mon, year == yr) x1 &lt;- prop.table(table(x1$subject_race)) x1 &lt;- as.data.frame(x1) x1 &lt;- filter(x1, Var1 == race)$Freq x2 &lt;- filter(SAN, light == &quot;night&quot;, month == mon, year == yr) x2 &lt;- prop.table(table(x2$subject_race)) x2 &lt;- as.data.frame(x2) x2 &lt;- filter(x2, Var1 == race)$Freq return(x1 - x2) } x &lt;- c() y &lt;- c() z &lt;- c() for (i in seq(2012, 2017)) { for (j in seq(1, 12)) { diff &lt;- racial_perc(j, i, &quot;black&quot;) x &lt;- c(x, diff) y &lt;- c(y, i) z &lt;- c(z, j) } } diff_data_SAN &lt;- as.data.frame(cbind(x, y, z)) diff_data_SAN &lt;- diff_data_SAN %&gt;% mutate(time = 1:nrow(diff_data_SAN)) x &lt;- c() y &lt;- c() z &lt;- c() for (i in seq(1, 4)) { diff &lt;- racial_perc(i, &quot;2018&quot;, &quot;black&quot;) x &lt;- c(x, diff) z &lt;- c(1, 2, 3, 4) y &lt;- c(2018, 2018, 2018, 2018) } n &lt;- nrow(diff_data_SAN) diff_data_SAN2 &lt;- as.data.frame(cbind(x, y, z)) diff_data_SAN2 &lt;- diff_data_SAN2 %&gt;% mutate(time = (n + 1):(n + 4)) diff_data_SAN &lt;- rbind(diff_data_SAN, diff_data_SAN2) ggplot(data = diff_data_SAN) + geom_point(mapping = aes(x = time, y = x)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) The policing data used for the visualization is from Sanantonio, Texas, and the date is from January 2012 to April 2018. This data recorded a total number of 1,040,428 observations. Key variables are the date of the stop made and the race of the driver, and the corresponding sunrise and sunset time were calculated with each of the given dates. If the stop took place after the sunrise time and before the sunset time, it was set to be a stop made during the day time. Stops that took place outside of this range were set to be night time stops. Then for each day, the percentage of black drivers stopped during day and night was separately calculated and their difference was documented. Finally, the visualization of the key variables was a scatter plot with the horizontal axis as the progression of time and the vertical axis as the day time percentage deducted by night time percentage. For us to claim that the plot has successfully captured the possible existence of racial discrimination, we should expect to observe the majority of the points to be above the horizontal line at zero. However, the scatter plot demonstrates a trend contrary to our expectations. Most of the points fall under the horizontal line, which indicates that for the region of Sanantonio, black drivers on average were stopped at a lower percentage during day time than night time. Using this method and the given data, we have failed the attempt to prove that there has been racial discrimination against black drivers within the data. One possible downfall of the aforementioned method is that it failed to acknowledge the possible difference in the distribution of driver populations between day time and night time. More specifically speaking, it is likely that for the region of Sanantonio, drivers who are on the road during night time and day time are two completely different population groups, and therefore, comparing the percentages of drivers stopped between these two possibly distinct population might be less meaningful. One possible improvement in the hope of alleviating population distinctions is the “veil of darkness” test, which limits to the stops made only one hour before and after the sunset time. This approach helps to reduce the possible distinction in the racial distribution of the night time and day time drivers. A similar set of procedures to the previous method was conducted, and a second scatter plot was produced. For the second time, the majority of the points are still below the horizontal line, which indicates that even if we have attempted to reduce the possible transformation of racial distribution, black drivers were mostly stopped at a higher percentage during night time relative to that of the day time. Therefore, the second plot has also failed to offer evidence for the existence of racial discrimination in the data for Sanantonio. SAN2 &lt;- SAN %&gt;% filter(sunset - 3600 &lt; date_time &amp; sunset + 3600 &gt; date_time) racial_perc2 &lt;- function(mon, yr, race) { x1 &lt;- filter(SAN2, light == &quot;day&quot;, month == mon, year == yr) x1 &lt;- prop.table(table(x1$subject_race)) x1 &lt;- as.data.frame(x1) x1 &lt;- filter(x1, Var1 == race)$Freq x2 &lt;- filter(SAN2, light == &quot;night&quot;, month == mon, year == yr) x2 &lt;- prop.table(table(x2$subject_race)) x2 &lt;- as.data.frame(x2) x2 &lt;- filter(x2, Var1 == race)$Freq return(x1 - x2) } x &lt;- c() y &lt;- c() z &lt;- c() for (i in seq(2012, 2017)) { for (j in seq(1, 12)) { diff &lt;- racial_perc(j, i, &quot;black&quot;) x &lt;- c(x, diff) y &lt;- c(y, i) z &lt;- c(z, j) } } diff_data_SAN &lt;- as.data.frame(cbind(x, y, z)) diff_data_SAN &lt;- diff_data_SAN %&gt;% mutate(time = 1:nrow(diff_data_SAN)) x &lt;- c() y &lt;- c() z &lt;- c() for (i in seq(1, 4)) { diff &lt;- racial_perc2(i, &quot;2018&quot;, &quot;black&quot;) x &lt;- c(x, diff) z &lt;- c(1, 2, 3, 4) y &lt;- c(2018, 2018, 2018, 2018) } n &lt;- nrow(diff_data_SAN) diff_data_SAN2 &lt;- as.data.frame(cbind(x, y, z)) diff_data_SAN2 &lt;- diff_data_SAN2 %&gt;% mutate(time = (n + 1):(n + 4)) diff_data_SAN &lt;- rbind(diff_data_SAN, diff_data_SAN2) ggplot(data = diff_data_SAN) + geom_point(mapping = aes(x = time, y = x)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) "],
["logistic-regression.html", "Chapter 6 Logistic Regression 6.1 Charlotte, NC Logistic Regression 6.2 Plotting S-curves 6.3 Veil of Darkness Nationwide 6.4 Predictive Models 6.5 Empirical Search Probabilities Nationwide", " Chapter 6 Logistic Regression 6.1 Charlotte, NC Logistic Regression Logistic regression models are commonly used for modeling the probability of an event that only takes two possible outcomes. The dependent variable should be binary, and the output parameters of the model should indicate how strongly will the independent variables affect the likelihood of the occurrence of the event. For the policing data, it would be of great interest to investigate how would race and age of drivers contribute to the likelihood of drivers being searched. The reason for choosing to inspect the likelihood of searches rather than stops is that, during night times, there indeed exists the concern that when the police made the stop, he or she might not know the driver’s identity. However, if the police are deciding between whether to search or not, he or she should have already approached the car and have a basic understanding of the driver’s race and age. It is also worthwhile to construct a logistic model that will present a visualization of how the likelihood of getting searched changes through time as well as the comparison between drivers with different racial identity. The policing data used for the regression analysis was collected from Charlotte, North Carolina with a time range from January, 2000 to October 2015. The binary dependent variable is “search”, which denotes whether the police have conducted a search. The key independent variables are the race and age of the driver. To better observe the effect of one race relative to the other and to eliminate possible confusions with omitted categories of variables, the data was limited to only include black and white drivers. Based on the notion of only two categorical group in the “search” variable, the logistic model in predicting the search probability is presented below. \\(age_i\\) is a numeric variable that denotes the age of the driver and \\(race_i\\) is a categorical variable that assigns value “1” to white drivers and “0” to black drivers. Inspecting each equation with respect to the race variable allows an interpretation of the parameters. The equation for \\(y_{white}\\) differs from \\(y_{black}\\) by a value of \\(\\beta_2 + \\beta_3 race_i\\) \\[\\begin{align*} y_i &amp;= \\beta_0 + \\beta_1 age_i + \\beta_2 race_i + \\beta_3 age_i*race_i \\\\ y_{white} &amp;= \\beta_0 + (\\beta_1 + \\beta_3) age_i + \\beta_2 race_i \\\\ y_{black} &amp;= \\beta_0 + \\beta_1 age \\end{align*}\\] Observing the output of the given model, \\(\\beta_2\\) and \\(\\beta_3\\) each has a value of -1.0199 and 0.00469, respectively. A negative \\(\\beta_2\\) indicates that during younger ages, white drivers might be less likely to be searched relative to black drivers. However, as drivers age, a positive \\(\\beta_3\\) indicates that the probability gap of getting searched between black and white drivers have shortened. ## ## Call: ## glm(formula = search ~ as.numeric(subject_age) * as.factor(subject_race), ## family = &quot;binomial&quot;, data = NCC2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6231 -0.4102 -0.3109 -0.2394 3.2771 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) -1.1158593 0.0138524 ## as.numeric(subject_age) -0.0424910 0.0004429 ## as.factor(subject_race)white -1.0206028 0.0261268 ## as.numeric(subject_age):as.factor(subject_race)white 0.0047084 0.0008116 ## z value Pr(&gt;|z|) ## (Intercept) -80.554 &lt; 2e-16 *** ## as.numeric(subject_age) -95.941 &lt; 2e-16 *** ## as.factor(subject_race)white -39.063 &lt; 2e-16 *** ## as.numeric(subject_age):as.factor(subject_race)white 5.802 6.57e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620366 on 1382621 degrees of freedom ## Residual deviance: 592472 on 1382618 degrees of freedom ## (4 observations deleted due to missingness) ## AIC: 592480 ## ## Number of Fisher Scoring iterations: 6 ## age group prob ## 1 -100.00 b 0.9582433 ## 2 -99.99 b 0.9582263 ## 3 -99.98 b 0.9582092 ## 4 -99.97 b 0.9581922 ## 5 -99.96 b 0.9581752 ## 6 -99.95 b 0.9581582 \\[\\begin{align*} Pr(search_i=1|black_i) &amp;= {\\frac{exp(\\beta_0 + \\beta_1 age_i)}{1 + exp (\\beta_0 + \\beta_1 age_i)}} \\\\ Pr(search_i=1|white_i) &amp;= {\\frac{exp(\\beta_0 + (\\beta_1 + \\beta_3)age_i + \\beta_2)}{1 + exp (\\beta_0 + (\\beta_1 + \\beta_3)age_i + \\beta_2)}} \\end{align*}\\] This plot shows the distribution of probabilities of being searched with the parameters of the logistic model. The horizontal axis represents the driver’s age and the vertical axis represents his or her likelihood of being searched. Two lines each represent the distribution for black and white drivers, and they are generated with equations stated above. It can be observed from this plot that when drivers are much younger (less than 35 years old), black drivers have a higher chance of being searched. When age increases, likelihood of being searched decreases for both groups of drivers. For Black drivers, the search probability seems to decrease at a higher rate relative to that of the white driver group. As the driver’s age increases to over approximately 50 years old, the two probabilities of getting searched seem to be relatively close to each other. One possible interpretation of this plot is that for the region of Charlotte, age does affect a driver’s likelihood of being searched, and additionally, the increase in the age of black driver has lowered his or her chance of being searched more so than that of the white drivers. 6.2 Plotting S-curves library(ggplot2) library(forcats) library(XML) library(RMySQL) library(tidyverse) library(rlist) library(lubridate) library(caret) library(rpart.plot) library(gridExtra) First let’s get the datasets that have our variables of interest all_dataset_names &lt;- as.list(DBI::dbGetQuery(con, &quot;SHOW TABLES&quot;))$Tables_in_traffic # all_dataset_names variables_of_interest &lt;- c(&quot;subject_age&quot;, &quot;subject_race&quot;, &quot;subject_sex&quot;, &quot;date&quot;, &quot;search_conducted&quot;) datasets_of_interest &lt;- relevant_datasets(all_dataset_names, variables_of_interest) datasets_of_interest &lt;- datasets_of_interest[-c(1, 3, 14, 21)] This section walks through the process of plotting nationwide s-curves. The next step for plotting multiple cities is querying the data from our datasets of interest. I then plotted both curves (black and white individuals) for each dataset. I do so by computing the probabilities \\(Pr(search_i=1|black_i)\\) and \\(Pr(search_i=1|white_i)\\). I use the values from the coefficient matrix to calculate the \\({\\beta_0 + \\beta_1 age_i}\\) coefficient for the black population and the \\({\\beta_0 + (\\beta_1 + \\beta_3)age_i + \\beta_2}\\) coefficient for the white population. The probabilities are then easily computed using the equation \\[{\\frac{exp(coefficient)}{1 + exp (coefficient)}}\\] for each population. More information on these probabilities can be found in the previous section. I removed the datasets from Pittsburg and Connecticut because the Pittsburg dataset seemed to have flipped search conducted data points (TRUE values were most likely actually FALSE values). I am more confident of this flaw given it was also stated in the Pierson et al. (2020) that this Pittsburg dataset may be corrupt. The Connecticut dataset was also removed after looking at the actual plotted probabilities for each age—there was extremely high variance and no trend, suggesting that the logistic regression model would not demonstrate any signal. coeff_matrix &lt;- read.csv(&quot;coeff_matrix_raw.csv&quot;) # function that uses our coefficient # matrix to compute probabilities for the # various datasets national_plot &lt;- function(num) { matrix &lt;- coeff_matrix[num, ] ages &lt;- seq(16, 90, 1) # chosing an appropriate age range coeff_b &lt;- (matrix$intercept) + (matrix$subject_age) * ages # calculating coefficients coeff_w &lt;- ((matrix$intercept) + (matrix$subject_race)) + (matrix$subject_age + matrix$subject_age.subject_race) * ages scurve_b &lt;- exp(coeff_b)/(1 + exp(coeff_b)) #computing probability curve from respective calculated coefficient scurve_w &lt;- exp(coeff_w)/(1 + exp(coeff_w)) plot_b &lt;- data.frame(prob = scurve_b, age = ages, state = matrix$state_abbreviation, race = &quot;black&quot;) plot_w &lt;- data.frame(prob = scurve_w, age = ages, state = matrix$state_abbreviation, race = &quot;white&quot;) plot_data &lt;- bind_rows(plot_b, plot_w) } combine_plots &lt;- lapply(seq(datasets_of_interest), national_plot) cities &lt;- do.call(&quot;rbind&quot;, combine_plots) # removed Pittsburg and Connecticut # statewide datasets data_b &lt;- cities %&gt;% filter(race == &quot;black&quot;) %&gt;% filter(state != &quot;PAPI&quot;) %&gt;% filter(state != &quot;CTH&quot;) data_w &lt;- cities %&gt;% filter(race == &quot;white&quot;) %&gt;% filter(state != &quot;PAPI&quot;) %&gt;% filter(state != &quot;CTH&quot;) # plotting the data ggplot() + geom_line(aes(x = age, y = prob, color = &quot;Black&quot;), data = data_b, lwd = 1) + geom_line(aes(x = age, y = prob, color = &quot;White&quot;), data = data_w, lwd = 1) + labs(x = &quot;age&quot;, y = &quot;Probability&quot;, color = &quot;Subject Race&quot;) + facet_wrap(~state) The nationwide graphs indicate that for both races, the probability of being searched decreases as age increase. Another observation is that black individuals have a higher probability of being searched for all ages in all except Philadelphia. I then plotted the separation between s-curves to visualize the difference in probabilities for black and white individuals. I did so by subtracting the s-curves generated by both coefficients (s-curve for black individuals - s-curve for black individuals). my_plot &lt;- ggplot() # filtering out Pittsburg and Connecticut coeff_matrix &lt;- coeff_matrix %&gt;% filter(state_abbreviation != &quot;PAPI&quot;) %&gt;% filter(state_abbreviation != &quot;CTH&quot;) # creating a singular plot for all # datasets for (num in 1:20) { matrix &lt;- coeff_matrix[num, ] ages &lt;- seq(16, 90, 1) coeff &lt;- (matrix$intercept) + (matrix$subject_age) * ages coeff2 &lt;- ((matrix$intercept) + (matrix$subject_race)) + (matrix$subject_age + matrix$subject_age.subject_race) * ages scurve1 &lt;- exp(coeff)/(1 + exp(coeff)) scurve2 &lt;- exp(coeff2)/(1 + exp(coeff2)) scurve &lt;- scurve1 - scurve2 #calculation of the difference in probability plot &lt;- data.frame(Probability = scurve, Age = ages, State = matrix$state_abbreviation) my_plot &lt;- my_plot + geom_line(plot, mapping = aes(x = Age, y = Probability, color = State)) } my_plot + theme(legend.text = element_text(size = 10), legend.position = &quot;bottom&quot;) + ggtitle(&quot;Difference in Probability of Being Searched for Black and White Individuals&quot;) This plot further demonstrates that the probability of being searched for black individuals is higher than white individuals for every age, with all curves except Philadelphia being entirely positive. The model also indicates that the difference in probabilities goes down as the individual’s age increases. The significance of the unique result from Philadelphia was not studied, but it is important to note that Philadelphia is in the same state as Pittsburg (the corrupt dataset), so there is a possibility of systematic issues influencing all datasets in the state. 6.2.1 Grouping After recognizing the difference in probabilities for black and white individuals, I became interested in what kind of characteristics of the state or city may influence the magnitude of this difference. I performed this priliminary investigation by simply hardcoding various factors about the state or city into the coefficient matrix. The first variable I used was how the state voted in the 2016 election. my_plot &lt;- ggplot() for (num in 1:20) { matrix &lt;- coeff_matrix[num, ] ages &lt;- seq(16, 90, 1) coeff &lt;- (matrix$intercept) + (matrix$subject_age) * ages coeff2 &lt;- ((matrix$intercept) + (matrix$subject_race)) + (matrix$subject_age + matrix$subject_age.subject_race) * ages scurve1 &lt;- exp(coeff)/(1 + exp(coeff)) scurve2 &lt;- exp(coeff2)/(1 + exp(coeff2)) scurve &lt;- scurve1 - scurve2 plot &lt;- data.frame(Probability = scurve, Age = ages, State = matrix$state_abbreviation, makeup = matrix$state_politics) my_plot &lt;- my_plot + geom_line(plot, mapping = aes(x = Age, y = Probability, color = makeup)) } my_plot + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;)) + theme(legend.text = element_text(size = 10)) + ggtitle(&quot;Subtracted S-curves by Blue or Red State&quot;) There is no apparant trend in this graph, suggesting that the magnitude of difference may not be influenced by state politics. I then continued to look at other variables. I was primarily interested in forms of diversity and used the ranking found at https://wallethub.com/edu/most-least-diverse-states-in-america/38262/#methodology as a measurement. Each state was given a rank out of 50, so I grouped the subtracted s-curves on a sliding scale based on these ranks. I hardcoded these characteristics into the coefficient matrix and plotted for all forms of diversity on the website. The plot for racial and ethnic diversity is shown here. my_plot &lt;- ggplot() for (num in 1:20) { matrix &lt;- coeff_matrix[num, ] ages &lt;- seq(16, 90, 1) coeff &lt;- (matrix$intercept) + (matrix$subject_age) * ages coeff2 &lt;- ((matrix$intercept) + (matrix$subject_race)) + (matrix$subject_age + matrix$subject_age.subject_race) * ages scurve1 &lt;- exp(coeff)/(1 + exp(coeff)) scurve2 &lt;- exp(coeff2)/(1 + exp(coeff2)) scurve &lt;- scurve1 - scurve2 plot &lt;- data.frame(Probability = scurve, Age = ages, State = matrix$state_abbreviation, makeup = matrix$state_politics, diversity = matrix$racial_and_ethnic) my_plot &lt;- my_plot + geom_line(plot, mapping = aes(x = Age, y = Probability, color = diversity)) } my_plot + scale_color_gradient(low = &quot;yellow&quot;, high = &quot;red&quot;) + theme(legend.text = element_text(size = 10)) + ggtitle(&quot;Subtracted S-curves by Racial and Ethnic Diversity Rankings&quot;) None of the variables presented any observable trend in the subtracted s-curves. However, note that this was only a simple priliminary analysis using one source for diversity ranking. In other words, this is by no means a complete study of these variables of diversity, so further work is encouraged to further study these variables as well as others. 6.3 Veil of Darkness Nationwide Running a veil of darkness logistic regression follows a similar process as using race and age as variables to predict a search being conducted. This time, we add a light binary variable First we want to get the relevant datasets. This time, we splice out different datasets as those are empty Here we apply the relevant_datasets function to get the character string. Then, we can use base R splicing to remove empty datasets. This code block defines the function, query_data, which calls the SQL command to retrieve the datasets with their speficied variables. Lastly, it will append each dataset to a list called datasets. At last, the previous four functions will be called in the clean_data function. Clean_data does two main things: 1) make search conducted into a binary datatype 2) add night and day variables to our dataset Notice how this is very similar to the previous clean_data function only this time we add a add_night_day function clean_data &lt;- function(i) { city_dataset &lt;- datasets[[i]] %&gt;% drop_na() # first add light and day variables tmp_lat &lt;- coordinates[[i]][1] tmp_long &lt;- coordinates[[i]][2] time_zone &lt;- lutz::tz_lookup_coords(tmp_lat, tmp_long, warn = F) city_dataset &lt;- add_night_day(city_dataset, time_zone, tmp_lat, tmp_long) # clean data if (typeof(city_dataset$search_conducted) == &quot;character&quot;) { city_dataset &lt;- city_dataset %&gt;% filter(subject_race == &quot;black&quot; | subject_race == &quot;white&quot;) %&gt;% mutate(search_conducted = case_when(search_conducted == &quot;TRUE&quot; ~ 1, search_conducted == &quot;FALSE&quot; ~ 0)) } else { # some datasets have search_conducted as # already a dbl city_dataset &lt;- city_dataset %&gt;% filter(subject_race == &quot;black&quot; | subject_race == &quot;white&quot;) %&gt;% mutate(search_conducted = search_conducted) } city_dataset &lt;- city_dataset %&gt;% mutate(subject_race = as.factor(case_when(subject_race == &quot;white&quot; ~ &quot;W&quot;, subject_race == &quot;black&quot; ~ &quot;B&quot;))) return(city_dataset) } datasets &lt;- lapply(seq(datasets), clean_data) fix_ages quickly sets any ages to a dbl data type fix_ages &lt;- function(city_dataset) { city_dataset &lt;- city_dataset %&gt;% mutate(subject_age = as.numeric(subject_age)) return(city_dataset) } datasets &lt;- lapply(datasets, fix_ages) Since we are analyzing multiple outputs from a logistic regression, we need a way to store those outputs. We decided to takes the coefficients from each logistic regression and make a dataframe out of them. coefficient_matrix &lt;- data.frame(intercept = numeric(), subject_raceW = as.numeric(), subject_age = as.numeric(), is_darkTRUE = as.numeric(), `subject_raceW:subject_age` = as.numeric(), dataset_name = character()) mapply(logistic_regression, datasets, datasets_of_interest) # fitlog &lt;- glm(formula = # search_conducted ~ # subject_race*subject_age+is_dark, data # = datasets[[1]], family = binomial, # control = list(maxit = 50)) # summary(fitlog) colnames(coefficient_matrix) &lt;- c(&quot;intercept&quot;, &quot;subject_raceW&quot;, &quot;subject_age&quot;, &quot;is_dark&quot;, &quot;subject_raceW.subject_age&quot;, &quot;datasetnames&quot;) Now that we have the coefficient_matrix we can select certain coefficients to compute the probabilities for white and black people along with day and night. \\(Pr(search_i=1|black_i)\\) and \\(Pr(search_i=1|white_i)\\). I use the values from the coefficient matrix to calculate the \\({\\beta_0 + \\beta_1 age_i}\\) coefficient for the black population in the day and \\({\\beta_0 + \\beta_1 age_i + \\beta_4}\\) for black population at night. \\({\\beta_0 + (\\beta_1 + \\beta_3)age_i + \\beta_2}\\) coefficient for the white population at day light, then \\({\\beta_0 + (\\beta_1 + \\beta_3)ages_i + \\beta_2 + \\beta_4}\\). The probabilities are then easily computed using the equation \\[{\\frac{exp(coefficient)}{1 + exp (coefficient)}}\\] for each population. More information on these probabilities can be found in the previous section. There are four s-curve we want to plot: Black driver in the day, Black and night, white and day, and White and night. Thus, we’ll make a dataframe for each scenario and then row bind all of them. plot_all &lt;- function(i) { matrix &lt;- coefficient_matrix[i, ] ages &lt;- seq(16, 90, 1) # black and day coefficients coeff_black &lt;- (matrix$intercept) + (matrix$subject_age) * ages scurve &lt;- exp(coeff_black)/(1 + exp(coeff_black)) plot.data &lt;- data.frame(prob = scurve, age = ages, state = as.character(matrix$datasetnames), race = as.character(&quot;black&quot;), is_dark = &quot;FALSE&quot;, stringsAsFactors = FALSE) # black and night coeff_black_night &lt;- (matrix$intercept) + (matrix$is_dark) + (matrix$subject_age) * ages scurve_night &lt;- exp(coeff_black_night)/(1 + exp(coeff_black_night)) plot.data_night &lt;- data.frame(prob = scurve_night, age = ages, state = as.character(matrix$datasetnames), race = as.character(&quot;black&quot;), is_dark = &quot;TRUE&quot;, stringsAsFactors = FALSE) # white and day coeff_white_day &lt;- matrix$intercept + matrix$subject_raceW + (matrix$subject_age + matrix$subject_raceW.subject_age) * ages scurve_white_day &lt;- exp(coeff_white_day)/(1 + exp(coeff_white_day)) plot.data_white_day &lt;- data.frame(prob = scurve_white_day, age = ages, state = as.character(matrix$datasetnames), race = as.character(&quot;white&quot;), is_dark = &quot;FALSE&quot;, stringsAsFactors = FALSE) # white and night coeff_white_night &lt;- matrix$intercept + matrix$subject_raceW + matrix$is_dark + (matrix$subject_age + matrix$subject_raceW.subject_age) * ages scurve_white_night &lt;- exp(coeff_white_night)/(1 + exp(coeff_white_night)) plot.data_white_night &lt;- data.frame(prob = scurve_white_night, age = ages, state = as.character(matrix$datasetnames), race = as.character(&quot;white&quot;), is_dark = &quot;TRUE&quot;, stringsAsFactors = FALSE) full_plot_data &lt;- bind_rows(plot.data_white_day, plot.data_white_night, plot.data, plot.data_night) } datum &lt;- lapply(seq(datasets), plot_all) all_cities &lt;- do.call(&quot;rbind&quot;, datum) all_cities &lt;- all_cities %&gt;% filter(state != &quot;PApittsburgh&quot; &amp; state != &quot;CThartford&quot;) Lastly, after getting the predicted probabilities from the different coefficients it is time to plot them. black_day_data &lt;- all_cities %&gt;% filter(race == &quot;black&quot; &amp; is_dark == &quot;FALSE&quot;) black_night_data &lt;- all_cities %&gt;% filter(race == &quot;black&quot; &amp; is_dark == &quot;TRUE&quot;) white_day_data &lt;- all_cities %&gt;% filter(race == &quot;white&quot; &amp; is_dark == &quot;FALSE&quot;) white_night_data &lt;- all_cities %&gt;% filter(race == &quot;white&quot; &amp; is_dark == &quot;TRUE&quot;) p &lt;- ggplot() + geom_line(aes(x = age, y = prob, color = &quot;black and day&quot;), data = black_day_data, lwd = 0.5) + geom_line(aes(x = age, y = prob, color = &quot;black and night&quot;), data = black_night_data, lwd = 0.5) + geom_line(aes(x = age, y = prob, color = &quot;white and day&quot;), data = white_day_data, lwd = 0.5) + geom_line(aes(x = age, y = prob, color = &quot;white and night&quot;), data = white_night_data, lwd = 0.5) + facet_wrap(~state) ggsave(&quot;cities_night_day_log_reg.png&quot;, p, scale = 15) The plots above show the probabilities of being searched given the subject’s race, in the day or night, and age. The most noteworthy thing to point out is that in nearly every prediction, white drivers at night tend to have a higher probability of getting searched. Moreover, in cities like Greensboro and Louisville, a driver being white or black has a higher probability of being searched rather at night than day. However, in all the cities black drivers at night still lead to the highest probabilities in being searched. This reiterates the Stanford Open Policing project’s caveat that the this test does account for seasonal changes, artificial lighting, and vehicle make. 6.4 Predictive Models durham &lt;- DBI::dbGetQuery(con, &quot;SELECT date, time, search_conducted, subject_race, subject_age, subject_sex, reason_for_stop FROM NCdurham&quot;) We look to predict whether a traffic stop results in a search using logistic regression. We use subject_age, subject_sex to predict search_conducted. We train the model on 100000 random samples, and then test the model on 200000 random samples from the durham dataset. # sampling training data new_durham &lt;- durham %&gt;% filter(subject_sex != &quot;NA&quot;) %&gt;% sample_n(1e+05) # sampling test data new_test &lt;- durham %&gt;% filter(subject_sex != &quot;NA&quot;) %&gt;% sample_n(2e+05) # makes model testmodel &lt;- glm(formula = as.factor(search_conducted) ~ as.numeric(subject_age) * as.factor(subject_sex), data = new_durham, family = &quot;binomial&quot;) # trains and tests model predtest &lt;- predict(testmodel, type = &quot;response&quot;) predict1 &lt;- predict(testmodel, newdata = new_durham, type = &quot;response&quot;) pred &lt;- ifelse(predict1 &gt; 0.05, &quot;TRUE&quot;, &quot;FALSE&quot;) # confusion matrix table(pred, new_durham$search_conducted) ## ## pred FALSE TRUE ## FALSE 46025 1337 ## TRUE 47254 5383 Using a confusion matrix, setting the percentage threshold to 0.05, we correctly predicted a search only 10% of the time. This suggests that the model is not doing a great job, but since there are a very small proportion of searched stops in the dataset, there might not be much signal to pickup. The model is unable to predict search conducted as well as we had hoped and so other tests, such as the threshold test could be used to help us answer a similar question. Regardless of how the model performs, we are still able to use the coefficients to help explain the data. 6.5 Empirical Search Probabilities Nationwide Here I plot the empirical search probabilities for each data set to see how the true search behavior compares with the predicted probabilities of our model. I use SQL commands COUNT and GROUP BY to circumvent downloading each data set individually; then, I use dplyr functions to calculate the search probability for each age and race group. (For race group, we only consider Black and white stopped motorists.) First I load the necessary packages and establish the SQL connection, which isn’t included here due to privacy. library(tidyverse) library(RMySQL) library(stringr) library(geofacet) library(grid) # for printing landscape plots 6.5.1 1. Create a list of relevant data sets Next, I use the function relevant_datasets to find the data set names that include all relevant variables: age, race, sex, date of stop, and whether or not a search was conducted. # query to find all data sets all_dataset_names &lt;- as.list(DBI::dbGetQuery(con, &quot;SHOW TABLES&quot;))$Tables_in_traffic variables_of_interest &lt;- c(&quot;subject_age&quot;, &quot;subject_race&quot;, &quot;subject_sex&quot;, &quot;date&quot;, &quot;search_conducted&quot;) datasets_of_interest &lt;- relevant_datasets(all_dataset_names, variables_of_interest) # manually remove certain data sets datasets_of_interest &lt;- datasets_of_interest[-c(1, 8, 20, 26, 30)] 6.5.2 2. Query the stop and search counts. Next, I use a function p_search_conducted to generate a list of data frames that has the probability that a search is conducted on a stopped motorist of a particular racial and age group. The input for this function is the name of a data set, and a data frame with search probabilities is returned. More details of how this function works is as follows. First, it checks the variable type of search_conducted because our SQL database contains data sets with search_conducted values as character strings (so, type varchar) or as a binary 0 or 1, (so, type double). Second, the function queries SQL twice, counting the number of times a search was conducted for each racial-age group and the number of times a stop was conducted for each racial-age group. Since our logistic regression considers only stops on Black and white people, p_search_conducted will only consider racial groups Black and white. The former is the numerator of the eventual search probability while the latter is the denominator. Lastly, with some dplyr data manipulation, I combine the two data frames resulting from the two SQL queries to have one data frame of search probabilities. I use the lapply function to iterate p_search_conducted on a list of data set names (and return a list of data frames.) p_search_conducted &lt;- function(dataset_name){ dataset_str &lt;- paste(dataset_name) # first, check the type of search_conducted explain_command_str &lt;- paste(&quot;EXPLAIN&quot;, dataset_name, sep = &quot; &quot;) explain_df &lt;- dbGetQuery(con, explain_command_str) %&gt;% filter(Field == &quot;search_conducted&quot;) %&gt;% mutate(dataset = dataset_str) %&gt;% select(Field, Type, dataset) # note that explain_df[1, 2] is the entry that has the type of search_conducted # second, create SQL search strings based on type of search_conducted if (explain_df[1, 2] == &quot;varchar(50)&quot;) { search_numerator_sql &lt;- paste(&quot;SELECT subject_age, subject_race, COUNT(*) as &#39;search_counts&#39; FROM&quot;, dataset_str, &quot;WHERE (subject_race=&#39;black&#39; OR subject_race = &#39;white&#39;) AND search_conducted = &#39;TRUE&#39; AND subject_age &gt; 0 GROUP BY subject_age, subject_race&quot;, sep = &quot; &quot;) } else if (explain_df[1, 2] == &quot;double&quot;) { search_numerator_sql &lt;- paste(&quot;SELECT subject_age, subject_race, COUNT(*) as &#39;search_counts&#39; FROM&quot;, dataset_str, &quot;WHERE (subject_race=&#39;black&#39; OR subject_race = &#39;white&#39;) AND search_conducted = &#39;1&#39; AND subject_age &gt; 0 GROUP BY subject_age, subject_race&quot;, sep = &quot; &quot;) } stops_denominator_sql &lt;- paste(&quot;SELECT subject_age, subject_race, COUNT(*) as &#39;total_stop_counts&#39; FROM&quot;, dataset_str, &quot;WHERE (subject_race=&#39;black&#39; OR subject_race = &#39;white&#39;) AND subject_age &gt; 0 GROUP BY subject_age, subject_race&quot;, sep = &quot; &quot;) # third, calculate % search_conducted per age in df thru query search_numerator &lt;- dbGetQuery(con, search_numerator_sql) stops_denominator &lt;- dbGetQuery(con, stops_denominator_sql) # fourth, combine results into one df search_probability &lt;- search_numerator %&gt;% right_join(stops_denominator, by = c(&quot;subject_race&quot;, &quot;subject_age&quot;)) %&gt;% replace_na(list(search_counts = 0)) %&gt;% mutate(search_percent = search_counts / total_stop_counts, # create column for dataset name dataset = dataset_str, subject_age = as.numeric(subject_age)) return(search_probability) } search_probs_list &lt;- lapply(datasets_of_interest, p_search_conducted) 6.5.3 3. Combine and plot the data Finally, I plot the data! combined_search_probs_list &lt;- bind_rows(search_probs_list, .id = &quot;column_label&quot;) empirical_search_p &lt;- combined_search_probs_list %&gt;% ggplot() + geom_point(mapping = aes(x = subject_age, y = search_percent, color = subject_race), alpha = 0.4) + facet_wrap(~dataset) + scale_y_continuous(limits = c(0, 0.4)) ggsave(&quot;empirical prob search_conducted.png&quot;, width = 14, height = 10, units = &quot;in&quot;) "],
["limitations-and-discussion.html", "Chapter 7 Limitations and Discussion", " Chapter 7 Limitations and Discussion In consideration of the exploratory work presented so far, we want to note various limitations that prevented us from conducting various analyses as well as finding particular results. The inconsistency of the data was by far the largest setback. For instance, not all data sets consisted of only traffic stop data sets, instead many data sets at the county or city level were a combination of traffic and pedestrian stop data sets. Only data sets at the state level or denoted as ‘State Patrol’ consisted only of traffic stops. This perhaps clouded a lot of the statistics and analyses we conducted as there is no efficient way currently to confidently parse all the pedestrian stops from the traffic stops. Another inconsistency was that information on the specific officer(s) on site of each stop was not as widely recorded. Previous literature suggested that the area of exploration was the behavior of particular officers instead of looking at an entire department. While this lead seemed promising, there simply is not enough data collected including officer information to analyze any trends or to make any inferences about individual police practices in regards to racial bias. "],
["references.html", "References", " References "]
]
